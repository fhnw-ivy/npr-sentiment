{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8be86aa",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Weak Labeling for Semi-Supervised Learning](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9ef403",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Weak Labeling for Semi-Supervised Learning](#toc1_)    \n",
    "  - [Setup](#toc1_1_)    \n",
    "  - [Load Datasets](#toc1_2_)    \n",
    "  - [Sentence Embeddings](#toc1_3_)    \n",
    "  - [Evaluation Function](#toc1_4_)    \n",
    "  - [Comparison between Embedding Models](#toc1_5_)    \n",
    "  - [Approach 1: Logistic Regression](#toc1_6_)    \n",
    "  - [Approach 2: KNeighbors Weak Labeling](#toc1_7_)    \n",
    "  - [Approach 2.1 KNeighbors Weak Labeling on PCA reduced embeddings](#toc1_8_)    \n",
    "  - [Approach 3: Random Forest](#toc1_9_)    \n",
    "  - [Approach 4: Neural Network](#toc1_10_)    \n",
    "  - [Approach 5: Support Vector Machine](#toc1_11_)    \n",
    "    - [Comparison of All Approaches](#toc1_11_1_)    \n",
    "    - [Summary](#toc1_11_2_)    \n",
    "  - [Save Best Models](#toc1_12_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fef04b",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use weak labeling for semi-supervised learning. We will explore different weak labeling techniques and evaluate their performance.\n",
    "\n",
    "The weak labeling techniques we will explore are:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. KNeighbors\n",
    "3. Random Forest\n",
    "4. Neural Network\n",
    "5. Support Vector Machine\n",
    "\n",
    "We will use the `sentence-transformers` library to generate sentence embeddings and the `scikit-learn` library to train the weak labeling models.\n",
    "\n",
    "The weak labeling models will be trained on the labeled development set and will then be used to predict the labels of the unlabeled development set. We will then evaluate the performance of the weak labeling models on the validation set, which is also labeled.\n",
    "\n",
    "Finally, we will save the best weak labeling models so that they can be used in the semi-supervised learning phase of the weak labeling together with the provided pipeline for inference.\n",
    "\n",
    "## <a id='toc1_1_'></a>[Setup](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdd2d9c6738180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from matplotlib import pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e139c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DATA_DIR = os.getenv('DATA_DIR', 'data')\n",
    "MODELS_DIR = os.getenv('MODELS_DIR', 'models')\n",
    "\n",
    "DATA_DIR = os.path.abspath(os.path.join(parent_dir, DATA_DIR))\n",
    "MODELS_DIR = os.path.abspath(os.path.join(parent_dir, MODELS_DIR))\n",
    "\n",
    "assert DATA_DIR is not None\n",
    "assert MODELS_DIR is not None\n",
    "\n",
    "SEED = 1337\n",
    "\n",
    "\n",
    "def set_seed():\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f620726b",
   "metadata": {},
   "source": [
    "As the `sentence-transformers` gives us a `torch` model, we need to check the environment to see if we can use a suitable accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c299fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Load Datasets](#toc0_)\n",
    "\n",
    "We will load the labeled development set, the unlabeled development set, and the validation set.\n",
    "\n",
    "Here a short overview of the datasets:\n",
    "- Train Dataset: Labeled development set\n",
    "- Validation Dataset: Validation set\n",
    "- Test Dataset: Unlabeled development set\n",
    "\n",
    "The test dataset would not be available in a real-world scenario, but we will use it to get an idea of how well the weak labeling models perform on unseen data and thus make assumptions about the performance later on the downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11fa9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loader import load_datasets\n",
    "\n",
    "partitions_dir = os.path.join(DATA_DIR, 'partitions')\n",
    "labelled_dev, unlabelled_dev, val_set = load_datasets(partitions_dir)\n",
    "train_df = labelled_dev\n",
    "y_train = train_df['label']\n",
    "\n",
    "val_df = val_set\n",
    "y_val = val_df['label']\n",
    "\n",
    "test_df = unlabelled_dev\n",
    "y_test = test_df['ground_truth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Sentence Embeddings](#toc0_)\n",
    "\n",
    "We will use the `sentence-transformers` library to generate sentence embeddings for the text data. The embedding models we will discuss are:\n",
    "1. `all-MiniLM-L6-v2`: Fast and efficient model\n",
    "2. `all-mpnet-base-v2`: High-quality model\n",
    "\n",
    "All the models are pre-trained on a large corpus of text data and can generate high-quality sentence embeddings. The choice of the model depends on the trade-off between quality and efficiency. From the documentation:\n",
    "\n",
    "> The all-mpnet-base-v2 model provides the best quality, while all-MiniLM-L6-v2 is 5 times faster and still offers good quality.\n",
    "\n",
    "In the next section, we will compare the impact on performance of the weak labeling models when using the two different embedding models and ultimately make a decision on which model to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a1eb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_transformer_model(model_name, device=device):\n",
    "    return SentenceTransformer(model_name).to(device)\n",
    "\n",
    "\n",
    "EMBEDDING_MODELS = {\n",
    "    'mini_lm': get_sentence_transformer_model(\"all-MiniLM-L6-v2\"),\n",
    "    'mpnet_base': get_sentence_transformer_model(\"all-mpnet-base-v2\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589e991",
   "metadata": {},
   "source": [
    "The functions below will be used to generate and save the sentence embeddings for the text data using the sentence-transformers model. The embeddings will be generated for the labeled development set, the unlabeled development set, and the validation set. The embeddings will be saved to disk so that they can be loaded later without having to regenerate them. \n",
    "\n",
    "If the embeddings are already generated and saved to disk, the functions will load the embeddings from disk instead of regenerating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca681c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "EMBEDDINGS_FOLDER = os.path.join(DATA_DIR, 'embeddings')\n",
    "os.makedirs(EMBEDDINGS_FOLDER, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    \"\"\"Load the embeddings from disk.\"\"\"\n",
    "    with open(f'{EMBEDDINGS_FOLDER}/{filename}.pkl', 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def generate_embeddings(model, texts, verbose=True):\n",
    "    \"\"\"Generate sentence embeddings using the sentence-transformers model.\"\"\"\n",
    "    embeddings = model.encode(texts, show_progress_bar=verbose)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def save_embeddings(embeddings, filename):\n",
    "    \"\"\"Save the embeddings to disk.\"\"\"\n",
    "    with open(f'{EMBEDDINGS_FOLDER}/{filename}.pkl', 'wb') as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "\n",
    "\n",
    "def generate_and_save_embeddings(model, texts, filename):\n",
    "    \"\"\"Generate and save the embeddings.\"\"\"\n",
    "    if os.path.exists(f'{EMBEDDINGS_FOLDER}/{filename}.pkl'):\n",
    "        embeddings = load_embeddings(filename)\n",
    "        return embeddings\n",
    "\n",
    "    embeddings = generate_embeddings(model, texts)\n",
    "    save_embeddings(embeddings, filename)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "EMBEDDING_MODEL_DATA = {}\n",
    "for model_name, model in EMBEDDING_MODELS.items():\n",
    "    print(f'Generating embeddings for {model_name}')\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Mappings back to partitioning keys:\n",
    "    # train -> labelled development set, \n",
    "    # val -> validation set\n",
    "    # test -> unlabelled development set\n",
    "    X_train = generate_and_save_embeddings(model, train_df['content'].values,\n",
    "                                           f'{model_name}/labelled_dev')\n",
    "    X_val = generate_and_save_embeddings(model, val_df['content'].values,\n",
    "                                         f'{model_name}/validation_set')\n",
    "    X_test = generate_and_save_embeddings(model, test_df['content'].values,\n",
    "                                          f'{model_name}/unlabelled_dev')\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'Generated embeddings for {model_name} in {end - start:.2f} seconds')\n",
    "    print(\n",
    "        f'Train Embeddings Shape: {X_train.shape}, Validation Embeddings Shape: {X_val.shape}, Test Embeddings Shape: {X_test.shape}')\n",
    "\n",
    "    EMBEDDING_MODEL_DATA[model_name] = {\n",
    "        'X_train': X_train,\n",
    "        'X_val': X_val,\n",
    "        'X_test': X_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff306f0",
   "metadata": {},
   "source": [
    "As expected the larger model `all-mpnet-base-v2` takes more time to generate the embeddings compared to the smaller model `all-MiniLM-L6-v2`. The dimensions of the embeddings also differ, with the larger model generating embeddings of size 768 and the smaller model generating embeddings of size 384."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Evaluation Function](#toc0_)\n",
    "\n",
    "We will use the following function to evaluate the performance of the weak labeling models on the validation and test sets. It will print the classification report and confusion matrix for the predictions made by the weak labeling models.\n",
    "\n",
    "The test set would not be available in a real-world scenario, but we will use it to get an idea of how well the weak labeling models perform on unseen data and thus make assumptions about the performance later on the downstream task. This obviously introduces some bias for the decision-making process, but we will use it for the sake of the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4067e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation(y_true, y_pred, title, plot_cm=False):\n",
    "    \"\"\"Prints the evaluation metrics for a classification model.\"\"\"\n",
    "    print(f'\\nClassification Report: {title}')\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    if plot_cm:\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix: {title}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Comparison between Embedding Models](#toc0_)\n",
    "In the process of comparing the performance of weak labeling approaches using different embedding models, we found that the choice of embedding significantly impacts model performance. For this reason, we compared the performance of the Logistic Regression model trained with embeddings generated by the two different models, all-MiniLM-L6-v2 and all-mpnet-base-v2.\n",
    "\n",
    "**Baseline Model: Logistic Regression**\n",
    "We used a simple Logistic Regression model for this baseline comparison to assess the impact of different embeddings on model performance. The Logistic Regression model was not tuned for hyperparameters in this initial phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c881f68daee748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "for model_name, model_data in EMBEDDING_MODEL_DATA.items():\n",
    "    X_train = model_data['X_train']\n",
    "    X_val = model_data['X_val']\n",
    "    X_test = model_data['X_test']\n",
    "\n",
    "    log_reg = LogisticRegression(max_iter=100_000)\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    print_evaluation(y_val, log_reg.predict(X_val),\n",
    "                     f'Logistic Regression (model={model_name}, set=Validation Set)')\n",
    "\n",
    "    print_evaluation(y_test, log_reg.predict(X_test),\n",
    "                     f'Logistic Regression (model={model_name}, set=Test Set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cb7529bf949011",
   "metadata": {},
   "source": [
    "The all-mpnet-base-v2 model outperformed the all-MiniLM-L6-v2 model with a notable margin in both validation and test sets. This improvement is consistent across all metrics, including precision, recall, F1-score, and accuracy. Specifically, the all-mpnet-base-v2 model shows an improvement of roughly 0.06 in F1-score on both the validation and test sets compared to the all-MiniLM-L6-v2 model.\n",
    "\n",
    "Given these results, we will use the embeddings generated by the all-mpnet-base-v2 model for subsequent steps. While this choice results in higher computational costs due to the larger dimensionality of the embeddings, the improved performance justifies this decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09acf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set X_train, X_val, X_test to the embeddings of the best model for subsequent steps\n",
    "X_train = EMBEDDING_MODEL_DATA['mpnet_base']['X_train']\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_val = EMBEDDING_MODEL_DATA['mpnet_base']['X_val']\n",
    "y_val = val_df['label']\n",
    "\n",
    "X_test = EMBEDDING_MODEL_DATA['mpnet_base']['X_test']\n",
    "y_test = test_df['ground_truth']\n",
    "\n",
    "print(f'X_train Shape: {X_train.shape}, y_train Shape: {y_train.shape}')\n",
    "print(f'X_val Shape: {X_val.shape}, y_val Shape: {y_val.shape}')\n",
    "print(f'X_test Shape: {X_test.shape}, y_test Shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[Approach 1: Logistic Regression](#toc0_)\n",
    "\n",
    "Logistic Regression is selected as the first approach due to its simplicity and interpretability, making it an ideal baseline for weak labeling tasks. It is a well-understood algorithm that provides clear insights into the relationship between features and the target variable. Despite its simplicity, Logistic Regression is effective for binary classification tasks, such as the one in this challenge. It serves as a solid starting point to evaluate the performance of weak labeling before exploring more complex models.\n",
    "\n",
    "We utilized grid search to find the best hyperparameters for the Logistic Regression model. The grid search was configured with 5-fold cross-validation and included eight different configurations, totaling 40 fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5106424381061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "log_reg_param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 3, 5, 10, 100],\n",
    "    'max_iter': [100_000]\n",
    "}\n",
    "\n",
    "log_reg_grid_search = GridSearchCV(LogisticRegression(random_state=SEED),\n",
    "                                   log_reg_param_grid,\n",
    "                                   n_jobs=-1,\n",
    "                                   cv=5,\n",
    "                                   verbose=3)\n",
    "log_reg_grid_search.fit(X_train, y_train)\n",
    "\n",
    "log_reg_model = log_reg_grid_search.best_estimator_\n",
    "print(f'Best Parameters: {log_reg_grid_search.best_params_}')\n",
    "\n",
    "print_evaluation(y_val, log_reg_model.predict(X_val),\n",
    "                 'Logistic Regression (Validation Set)')\n",
    "\n",
    "print_evaluation(y_test, log_reg_model.predict(X_test),\n",
    "                 'Logistic Regression (Test Set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb12ca",
   "metadata": {},
   "source": [
    "- **Balanced Performance**: Consistent scores across both classes demonstrate the model's ability to handle binary classification effectively.\n",
    "- **High Reliability**: An accuracy of 0.82-0.83 across validation and test datasets reflects the model's reliable prediction capabilities.\n",
    "- **Good Generalization**: Similar performance on both datasets suggests that the model generalizes well to new data, which is crucial for the practical application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[Approach 2: KNeighbors Weak Labeling](#toc0_)\n",
    "\n",
    "The KNeighbors classifier is chosen for its intuitive approach to classification by considering the proximity of data points in the feature space. This method is particularly suitable for weak labeling as it leverages the notion of clustering, which is analogous to finding the nearest neighbors for a given data point. By using Grid Search to tune hyperparameters, we aim to identify the optimal configuration that can enhance the performance of the weak labeling model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0512b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(),\n",
    "                           param_grid,\n",
    "                           n_jobs=-1,\n",
    "                           cv=5,\n",
    "                           verbose=3)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "knn_model = grid_search.best_estimator_\n",
    "\n",
    "print_evaluation(y_val, knn_model.predict(X_val),\n",
    "                 'KNeighbors Weak Labeling (Validation Set)')\n",
    "\n",
    "print_evaluation(y_test, knn_model.predict(X_test),\n",
    "                 'KNeighbors Weak Labeling (Test Set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96d2a3e",
   "metadata": {},
   "source": [
    "The KNeighbors classifier shows good performance on the validation set, with a slight decrease in accuracy on the test set. It displays a higher recall for class 0 and higher precision for class 1, indicating an effective balance in recognizing both classes but with a slight bias towards class 0 in terms of recall. This might be beneficial in scenarios where the cost of false negatives is higher for class 0.\n",
    "\n",
    "\n",
    "- **Class Detection**: The model is more effective at identifying class 0 with higher recall, beneficial for tasks where missing out on this class could be more detrimental.\n",
    "- **Precision-Recall Trade-off**: While precision is slightly higher for class 1, the recall is lower, suggesting a need to manage this trade-off based on specific task requirements.\n",
    "- **Moderate Generalization**: The slight decline in accuracy from validation to test set suggests room for improvement in generalizing to unseen data.\n",
    "- **Dimensionality Challenges**: Performance suggests handling of high-dimensional data is effective, though some variability in recall and precision indicates potential challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_8_'></a>[Approach 2.1 KNeighbors Weak Labeling on PCA reduced embeddings](#toc0_)\n",
    "\n",
    "To address the limitations of the KNeighbors classifier in high-dimensional spaces, we incorporate Principal Component Analysis (PCA) to reduce the dimensionality of the sentence embeddings. This step is justified by the fact that KNeighbors can struggle with high-dimensional data, leading to the curse of dimensionality. PCA helps in capturing the most significant features, thereby improving the efficiency and performance of the KNeighbors classifier in the weak labeling task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed1000c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=8)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "knn_pca_model = grid_search.best_estimator_\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "\n",
    "print_evaluation(y_val, knn_pca_model.predict(pca.transform(X_val)),\n",
    "                 'KNeighbors Weak Labeling (PCA Reduced) (Validation Set)')\n",
    "\n",
    "print_evaluation(y_test, knn_pca_model.predict(pca.transform(X_test)),\n",
    "                 'KNeighbors Weak Labeling (PCA Reduced) (Test Set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebb2e17",
   "metadata": {},
   "source": [
    "The application of PCA has slightly reduced the performance metrics compared to the original KNeighbors approach without PCA, indicating a possible loss of important information during the dimensionality reduction process. However, the model still performs adequately with balanced metrics for precision, recall, and F1-score, demonstrating its capability to maintain a decent level of classification accuracy.\n",
    "\n",
    "- **Dimensionality Reduction Impact**: Reducing dimensions has led to a uniform performance across classes but at a slightly reduced level compared to the non-PCA approach.\n",
    "- **Class Detection**: The model shows nearly equal performance in detecting both classes, with a slight favor towards class 1 in terms of recall.\n",
    "- **Overall Accuracy**: An accuracy of around 0.76-0.77 indicates that while PCA helps in managing high-dimensional data, it might also lead to the exclusion of some relevant features.\n",
    "- **Balanced Metrics**: Precision, recall, and F1-score all hover around 0.76-0.77, showing that the model performs equally well across different metrics but with a moderate level of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[Approach 3: Random Forest](#toc0_)\n",
    "\n",
    "Random Forest is selected for its robustness and ability to handle complex data through ensemble learning. This approach justifies its use by combining multiple decision trees to improve classification accuracy and prevent overfitting. Random Forests are particularly effective in capturing nonlinear relationships and interactions between features, making them a powerful choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c34640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "gradient_boosting_param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [3, 5, 7, 9, 11, 13]\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(RandomForestClassifier(),\n",
    "                              gradient_boosting_param_grid,\n",
    "                              n_jobs=-1,\n",
    "                              cv=5,\n",
    "                              verbose=3)\n",
    "\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "rf_model = rf_grid_search.best_estimator_\n",
    "print(f'Best Parameters: {rf_grid_search.best_params_}')\n",
    "\n",
    "print_evaluation(y_val, rf_model.predict(X_val),\n",
    "                 'Random Forest (Validation Set)')\n",
    "\n",
    "print_evaluation(y_test, rf_model.predict(X_test),\n",
    "                 'Random Forest (Test Set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db39d083",
   "metadata": {},
   "source": [
    "The Random Forest model shows excellent performance on the validation set with high precision, recall, and F1-score, indicating a strong ability to differentiate between classes effectively. The slightly lower performance on the test set still reflects good generalization capabilities, though there is a slight drop in all metrics compared to the validation set. The model shows a higher recall for class 0 and higher precision for class 1, suggesting an effective balance in recognizing the importance of both reducing false negatives for class 0 and false positives for class 1.\n",
    "\n",
    "- **Balanced Performance**: The model achieves balanced precision, recall, and F1-score, indicating it can accurately classify both positive and negative instances without significant bias.\n",
    "- **High Reliability**: An accuracy of around 0.81/0.85 reflects the model's strong ability to make correct predictions across both validation and test sets.\n",
    "- **Good Generalization**: The performance on both the validation and test sets suggests that the model generalizes well to new, unseen data.\n",
    "- **Effective Ensemble Learning**: The consistent and high performance metrics across different data sets reflect the robustness of Random Forest's ensemble learning approach, which combines multiple decision trees to improve classification accuracy and prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_10_'></a>[Approach 4: Neural Network](#toc0_)\n",
    "\n",
    "Neural Networks are chosen for their capability to model complex patterns and relationships in the data. This approach is justified by the neural network's ability to learn from high-dimensional sentence embeddings and possibly capture intricate feature representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8507f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_param_grid = {\n",
    "    'hidden_layer_sizes': [(1024, 512), (512, 128), (256, 64), (128, 32)],\n",
    "    'max_iter': [10000],\n",
    "}\n",
    "\n",
    "mlp_grid_search = GridSearchCV(MLPClassifier(),\n",
    "                               mlp_param_grid,\n",
    "                               n_jobs=-1,\n",
    "                               cv=5,\n",
    "                               verbose=3)\n",
    "mlp_grid_search.fit(X_train, y_train)\n",
    "mlp_model = mlp_grid_search.best_estimator_\n",
    "\n",
    "plt.plot(mlp_model.loss_curve_)\n",
    "plt.title('Neural Network Loss Curve')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "print(f'Best Parameters: {mlp_grid_search.best_params_}')\n",
    "\n",
    "print_evaluation(y_val, mlp_model.predict(X_val),\n",
    "                 'Neural Network (Validation Set)')\n",
    "\n",
    "print_evaluation(y_test, mlp_model.predict(X_test),\n",
    "                 'Neural Network (Test Set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2dadfc",
   "metadata": {},
   "source": [
    "The Neural Network model displays strong and consistent performance on the test set, slightly outperforming its validation set results. The model shows a higher recall for class 0 and higher precision for class 1 on both sets, indicating its effective balance in minimizing false negatives for class 0 and false positives for class 1. The results suggest that the neural network can accurately classify both positive and negative instances without significant bias.\n",
    "\n",
    "- **Balanced Performance**: Precision, recall, and F1-score are consistent, demonstrating the neural network's ability to classify accurately across different metrics.\n",
    "- **High Accuracy**: An accuracy of around 0.79/0.82 across validation and test datasets reflects the model's robust ability to make correct predictions.\n",
    "- **Good Generalization**: Similar performance on both the validation and test sets suggests that the neural network generalizes well to unseen data, an essential trait for real-world applications.\n",
    "- **Effective Feature Representation**: The balanced and high performance metrics indicate that the neural network is effectively capturing complex patterns and relationships in the data, leveraging its architecture to provide robust predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_11_'></a>[Approach 5: Support Vector Machine](#toc0_)\n",
    "\n",
    "Support Vector Machines (SVM) are included for their effectiveness in high-dimensional spaces and robustness against overfitting. SVMs are particularly suitable for weak labeling as they can find the optimal hyperplane that maximizes the margin between different classes. This approach is justified by the SVM's strong theoretical foundation and its ability to handle both linear and nonlinear classification tasks through different kernel functions, ensuring versatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e5ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_param_grid = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1, 3, 5, 10],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "svm_grid_search = GridSearchCV(SVC(),\n",
    "                               svm_param_grid,\n",
    "                               n_jobs=-1,\n",
    "                               cv=5,\n",
    "                               verbose=3)\n",
    "\n",
    "svm_grid_search.fit(X_train, y_train)\n",
    "svm_model = svm_grid_search.best_estimator_\n",
    "print(f'Best Parameters: {svm_grid_search.best_params_}')\n",
    "\n",
    "print_evaluation(y_val, svm_model.predict(X_val),\n",
    "                 'Support Vector Machine (Validation Set)')\n",
    "\n",
    "print_evaluation(y_test, svm_model.predict(X_test),\n",
    "                 'Support Vector Machine (Test Set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16769a04",
   "metadata": {},
   "source": [
    "The SVM model shows strong and consistent performance across both the validation and test sets, effectively maintaining its precision, recall, F1-score, and accuracy. The slight advantage in precision for class 1 and better recall for class 0 on the validation set indicates the model's capability to balance effectively between minimizing false positives and false negatives, a critical aspect for weak labeling tasks.\n",
    "\n",
    "- **High Precision and Recall**: Precision and recall are both consistently at 0.82 across both classes, indicating the SVM's effectiveness in correctly identifying both positive and negative instances without bias.\n",
    "- **Consistent F1-Score**: An F1-score of 0.82 for both classes shows that the model maintains a balance between precision and recall, ensuring reliable performance.\n",
    "- **High Accuracy**: An accuracy of 0.82 on both validation and test sets suggests that the SVM is highly reliable and capable of making correct predictions consistently.\n",
    "- **Good Generalization**: The similar performance metrics on both the validation and test sets imply that the SVM generalizes well to unseen data, making it a robust choice for real-world applications.\n",
    "- **Effective Margin Maximization**: The strong and balanced performance of the SVM reflects its ability to effectively find the optimal hyperplane that maximizes the margin between classes, ensuring robust classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f059b05d",
   "metadata": {},
   "source": [
    "### <a id='toc1_11_1_'></a>[Comparison of All Approaches](#toc0_)\n",
    "\n",
    "- **Logistic Regression**\n",
    "  - **Performance**: Achieves balanced metrics with an accuracy of 0.82-0.83, demonstrating robustness in binary classification.\n",
    "  - **Strengths**: Simple, computationally efficient, and offers good generalization.\n",
    "\n",
    "- **KNeighbors**\n",
    "  - **Performance**: Shows good precision and recall around 0.79-0.82, with slightly lower performance when using PCA.\n",
    "  - **Strengths**: Effective for identifying class-specific characteristics; performance may vary with dimensionality changes.\n",
    "\n",
    "- **Random Forest**\n",
    "  - **Performance**: Maintains high performance with precision, recall, and accuracy around 0.81-0.85, showing strong classification capabilities.\n",
    "  - **Strengths**: Robust against overfitting and capable of handling complex data structures through ensemble learning.\n",
    "\n",
    "- **Neural Network**\n",
    "  - **Performance**: Exhibits a stable performance with metrics generally around 0.79-0.82, indicating good pattern recognition and data fitting.\n",
    "  - **Strengths**: Captures complex relationships in data, providing depth in feature learning.\n",
    "\n",
    "- **Support Vector Machine (SVM)**\n",
    "  - **Performance**: Consistently delivers high precision and recall of 0.82, demonstrating effective margin maximization between classes.\n",
    "  - **Strengths**: Excels in high-dimensional spaces and shows excellent generalization capabilities.\n",
    "\n",
    "### <a id='toc1_11_2_'></a>[Summary](#toc0_)\n",
    "- **Top Performers**: SVM and Logistic Regression stand out for their high accuracy and strong generalization across both validation and test sets.\n",
    "- **Robust Choices**: Random Forest and Neural Network also show impressive performance, particularly in handling complex datasets and learning detailed data patterns.\n",
    "- **Special Considerations**: KNeighbors demonstrates good performance. When using PCA for dimensionality reduction, there is a slight decline in performance, indicating that while PCA can simplify the feature space, it may also omit critical features necessary for optimal class distinction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_12_'></a>[Save Best Models](#toc0_)\n",
    "\n",
    "For the semi-supervised learning phase of the weak labeling, we will save the best weak labeling models. The saved models can be used for inference in the `weak_labelling.py` script. For an example usage of the weak labeling pipeline, refer to the `README.md` file.\n",
    "\n",
    "Although we will not all the models in the final pipeline, we will save them for reference and potential future use and completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d946e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_FOLDER = os.getenv('MODELS_DIR', 'models')\n",
    "os.makedirs(MODELS_FOLDER, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_model(model, filename, overwrite=False):\n",
    "    if os.path.exists(f'{MODELS_FOLDER}/weak_labelling/{filename}.pkl') and not overwrite:\n",
    "        print(f'{filename} already exists. Skipping...')\n",
    "        return\n",
    "\n",
    "    with open(f'{MODELS_FOLDER}/weak_labelling/{filename}.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "\n",
    "save_model(log_reg_model, 'log_reg')\n",
    "save_model(knn_model, 'knn')\n",
    "save_model(knn_pca_model, 'knn_pca')\n",
    "save_model(rf_model, 'rf')\n",
    "save_model(mlp_model, 'mlp')\n",
    "save_model(svm_model, 'svm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
