{
 "cells": [
  {
   "cell_type": "raw",
   "id": "41fc4ba2",
   "metadata": {},
   "source": [
    "---\n",
    "title: Weak Labeling for Semi-Supervised Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a871a",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use weak labeling for semi-supervised learning. We will explore different weak labeling techniques and evaluate their performance.\n",
    "\n",
    "The weak labeling techniques we will explore are:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. KNeighbors\n",
    "3. Random Forest\n",
    "4. Neural Network\n",
    "5. Support Vector Machine\n",
    "\n",
    "We will use the `sentence-transformers` library to generate sentence embeddings and the `scikit-learn` library to train the weak labeling models.\n",
    "\n",
    "The weak labeling models will be trained on the labeled development set and will then be used to predict the labels of the unlabeled development set. We will then evaluate the performance of the weak labeling models on the validation set, which is also labeled.\n",
    "\n",
    "Finally, we will save the best weak labeling models so that they can be used in the semi-supervised learning phase of the weak labeling together with the provided pipeline for inference.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from matplotlib import pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ],
   "id": "7fdd2d9c6738180d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e139c3f1",
   "metadata": {},
   "source": [
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DATA_DIR = os.getenv('DATA_DIR', 'data')\n",
    "MODELS_DIR = os.getenv('MODELS_DIR', 'models')\n",
    "\n",
    "DATA_DIR = os.path.abspath(os.path.join(parent_dir, DATA_DIR))\n",
    "MODELS_DIR = os.path.abspath(os.path.join(parent_dir, MODELS_DIR))\n",
    "\n",
    "assert DATA_DIR is not None\n",
    "assert MODELS_DIR is not None\n",
    "\n",
    "SEED = 1337\n",
    "\n",
    "\n",
    "def set_seed():\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f620726b",
   "metadata": {},
   "source": [
    "As the `sentence-transformers` gives us a `torch` model, we need to check the environment to see if we can use a suitable accelerator."
   ]
  },
  {
   "cell_type": "code",
   "id": "3d1f1761",
   "metadata": {},
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Device:', device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "277bff21",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "We will load the labeled development set, the unlabeled development set, and the validation set.\n",
    "\n",
    "Here a short overview of the datasets:\n",
    "- Train Dataset: Labeled development set\n",
    "- Validation Dataset: Validation set\n",
    "- Test Dataset: Unlabeled development set\n",
    "\n",
    "The test dataset would not be available in a real-world scenario, but we will use it to get an idea of how well the weak labeling models perform on unseen data and thus make assumptions about the performance later on the downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "id": "973b1e3b",
   "metadata": {},
   "source": [
    "from src.data_loader import load_datasets\n",
    "\n",
    "partitions_dir = os.path.join(DATA_DIR, 'partitions')\n",
    "labelled_dev, unlabelled_dev, val_set = load_datasets(partitions_dir)\n",
    "train_df = labelled_dev\n",
    "y_train = train_df['label']\n",
    "\n",
    "val_df = val_set\n",
    "y_val = val_df['label']\n",
    "\n",
    "test_df = unlabelled_dev\n",
    "y_test = test_df['ground_truth']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "75f61f02",
   "metadata": {},
   "source": [
    "## Sentence Embeddings\n",
    "\n",
    "We will use the `sentence-transformers` library to generate sentence embeddings for the text data. The embedding models we will discuss are:\n",
    "1. `all-MiniLM-L6-v2`: Fast and efficient model\n",
    "2. `all-mpnet-base-v2`: High-quality model\n",
    "\n",
    "All the models are pre-trained on a large corpus of text data and can generate high-quality sentence embeddings. The choice of the model depends on the trade-off between quality and efficiency. From the documentation:\n",
    "\n",
    "> The all-mpnet-base-v2 model provides the best quality, while all-MiniLM-L6-v2 is 5 times faster and still offers good quality.\n",
    "\n",
    "In the next section, we will compare the impact on performance of the weak labeling models when using the two different embedding models and ultimately make a decision on which model to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "71a1eb27",
   "metadata": {},
   "source": [
    "def get_sentence_transformer_model(model_name, device=device):\n",
    "    return SentenceTransformer(model_name).to(device)\n",
    "\n",
    "\n",
    "EMBEDDING_MODELS = {\n",
    "    'mini_lm': get_sentence_transformer_model(\"all-MiniLM-L6-v2\"),\n",
    "    'mpnet_base': get_sentence_transformer_model(\"all-mpnet-base-v2\")\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6589e991",
   "metadata": {},
   "source": [
    "The functions below will be used to generate and save the sentence embeddings for the text data using the sentence-transformers model. The embeddings will be generated for the labeled development set, the unlabeled development set, and the validation set. The embeddings will be saved to disk so that they can be loaded later without having to regenerate them. \n",
    "\n",
    "If the embeddings are already generated and saved to disk, the functions will load the embeddings from disk instead of regenerating them."
   ]
  },
  {
   "cell_type": "code",
   "id": "ca681c1b",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "EMBEDDINGS_FOLDER = os.path.join(DATA_DIR, 'embeddings')\n",
    "os.makedirs(EMBEDDINGS_FOLDER, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    \"\"\"Load the embeddings from disk.\"\"\"\n",
    "    with open(f'{EMBEDDINGS_FOLDER}/{filename}.pkl', 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def generate_embeddings(model, texts, verbose=True):\n",
    "    \"\"\"Generate sentence embeddings using the sentence-transformers model.\"\"\"\n",
    "    embeddings = model.encode(texts, show_progress_bar=verbose)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def save_embeddings(embeddings, filename):\n",
    "    \"\"\"Save the embeddings to disk.\"\"\"\n",
    "    with open(f'{EMBEDDINGS_FOLDER}/{filename}.pkl', 'wb') as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "\n",
    "\n",
    "def generate_and_save_embeddings(model, texts, filename):\n",
    "    \"\"\"Generate and save the embeddings.\"\"\"\n",
    "    if os.path.exists(f'{EMBEDDINGS_FOLDER}/{filename}.pkl'):\n",
    "        embeddings = load_embeddings(filename)\n",
    "        return embeddings\n",
    "\n",
    "    embeddings = generate_embeddings(model, texts)\n",
    "    save_embeddings(embeddings, filename)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "EMBEDDING_MODEL_DATA = {}\n",
    "for model_name, model in EMBEDDING_MODELS.items():\n",
    "    print(f'Generating embeddings for {model_name}')\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Mappings back to partitioning keys:\n",
    "    # train -> labelled development set, \n",
    "    # val -> validation set\n",
    "    # test -> unlabelled development set\n",
    "    X_train = generate_and_save_embeddings(model, train_df['content'].values,\n",
    "                                           f'{model_name}/labelled_dev')\n",
    "    X_val = generate_and_save_embeddings(model, val_df['content'].values,\n",
    "                                         f'{model_name}/validation_set')\n",
    "    X_test = generate_and_save_embeddings(model, test_df['content'].values,\n",
    "                                          f'{model_name}/unlabelled_dev')\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'Generated embeddings for {model_name} in {end - start:.2f} seconds')\n",
    "    print(\n",
    "        f'Train Embeddings Shape: {X_train.shape}, Validation Embeddings Shape: {X_val.shape}, Test Embeddings Shape: {X_test.shape}')\n",
    "\n",
    "    EMBEDDING_MODEL_DATA[model_name] = {\n",
    "        'X_train': X_train,\n",
    "        'X_val': X_val,\n",
    "        'X_test': X_test\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As expected the larger model `all-mpnet-base-v2` takes more time to generate the embeddings compared to the smaller model `all-MiniLM-L6-v2`. The dimensions of the embeddings also differ, with the larger model generating embeddings of size 768 and the smaller model generating embeddings of size 384.",
   "id": "e5b796d34c73ee95"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluation Function\n",
    "\n",
    "We will use the following function to evaluate the performance of the weak labeling models on the validation and test sets. It will print the classification report and confusion matrix for the predictions made by the weak labeling models.\n",
    "\n",
    "The test set would not be available in a real-world scenario, but we will use it to get an idea of how well the weak labeling models perform on unseen data and thus make assumptions about the performance later on the downstream task. This obviously introduces some bias for the decision-making process, but we will use it for the sake of the demonstration."
   ],
   "id": "a8b1df01b77b4bb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def print_evaluation(y_true, y_pred, title, plot_cm=False):\n",
    "    \"\"\"Prints the evaluation metrics for a classification model.\"\"\"\n",
    "    print(f'\\nClassification Report: {title}')\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    if plot_cm:\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix: {title}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()"
   ],
   "id": "11bddb1af135a2ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comparison between Embedding Models\n",
    "\n",
    "We discovered in the process of working on the comparison of weak labelling approaches, that the difference in the embeddings generated by the two models significantly affects the performance of the weak labeling models. For this reason, we will compare the performance of the weak labeling models trained on the embeddings generated by the two models to get an idea of which model performs better. We will make a decision afterwards on which model to use for all the subsequent approaches discussed. This as the computational cost would otherwise be excessive also considering the downstream tasks of fine-tuning and transfer learning for both embedding models used to generate weak labels. \n",
    "\n",
    "We will use a simple Logistic Regression model for this comparison. As this is a baseline comparison, we will not perform hyperparameter tuning for the Logistic Regression model."
   ],
   "id": "3262dad9a749aa04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "for model_name, model_data in EMBEDDING_MODEL_DATA.items():\n",
    "    X_train = model_data['X_train']\n",
    "    X_val = model_data['X_val']\n",
    "    X_test = model_data['X_test']\n",
    "\n",
    "    log_reg = LogisticRegression(max_iter=100_000)\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    print_evaluation(y_val, log_reg.predict(X_val),\n",
    "                     f'Logistic Regression (model={model_name}, set=Validation Set)')\n",
    "\n",
    "    print_evaluation(y_test, log_reg.predict(X_test),\n",
    "                     f'Logistic Regression (model={model_name}, set=Test Set)')"
   ],
   "id": "4c881f68daee748a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can see that for this baseline comparison the `all-mpnet-base-v2` model performs better than the `all-MiniLM-L6-v2` model with a considerable margin of roughly 0.06% in F1-score on both the validation and test sets.\n",
    "\n",
    "As the `all-mpnet-base-v2` model performs better than the `all-MiniLM-L6-v2` model, we will use the embeddings generated by the `all-mpnet-base-v2` model for the subsequent steps although the dimensionality of the embeddings is higher. This could result in higher computational costs but give the observed margin in performance this seems justified."
   ],
   "id": "62cb7529bf949011"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# set X_train, X_val, X_test to the embeddings of the best model for subsequent steps\n",
    "X_train = EMBEDDING_MODEL_DATA['mpnet_base']['X_train']\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_val = EMBEDDING_MODEL_DATA['mpnet_base']['X_val']\n",
    "y_val = val_df['label']\n",
    "\n",
    "X_test = EMBEDDING_MODEL_DATA['mpnet_base']['X_test']\n",
    "y_test = test_df['ground_truth']\n",
    "\n",
    "print(f'X_train Shape: {X_train.shape}, y_train Shape: {y_train.shape}')\n",
    "print(f'X_val Shape: {X_val.shape}, y_val Shape: {y_val.shape}')\n",
    "print(f'X_test Shape: {X_test.shape}, y_test Shape: {y_test.shape}')"
   ],
   "id": "8f4c6bd0c28e07e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Approach 1: Logistic Regression\n",
    "\n",
    "Logistic Regression is selected as the first approach due to its simplicity and interpretability, making it an ideal baseline for weak labeling tasks. It is a well-understood algorithm that provides clear insights into the relationship between features and the target variable. Despite its simplicity, Logistic Regression is effective for binary classification tasks, such as the one in this challenge. It serves as a solid starting point to evaluate the performance of weak labeling before exploring more complex models.\n",
    "\n",
    "We will use grid search to find the best hyperparameters for the logistic regression model as we will do for all the other subsequent approaches discussed in this notebook."
   ],
   "id": "4c609bbc500a0184"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "log_reg_param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 3, 5, 10, 100],\n",
    "    'max_iter': [100_000]\n",
    "}\n",
    "\n",
    "log_reg_grid_search = GridSearchCV(LogisticRegression(random_state=SEED),\n",
    "                                   log_reg_param_grid,\n",
    "                                   n_jobs=-1,\n",
    "                                   cv=5,\n",
    "                                   verbose=3)\n",
    "log_reg_grid_search.fit(X_train, y_train)\n",
    "\n",
    "log_reg_model = log_reg_grid_search.best_estimator_\n",
    "print(f'Best Parameters: {log_reg_grid_search.best_params_}')\n",
    "\n",
    "print_evaluation(y_val, log_reg_model.predict(X_val),\n",
    "                 'Logistic Regression (Validation Set)')\n",
    "\n",
    "print_evaluation(y_test, log_reg_model.predict(X_test),\n",
    "                 'Logistic Regression (Test Set)')"
   ],
   "id": "38b5106424381061",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Balanced Performance**: The model shows consistent precision, recall, and F1-score of 0.86 across both classes, indicating it can accurately classify both positive and negative instances without bias.\n",
    "- **High Reliability**: An accuracy of 0.86 reflects the model's strong ability to make correct predictions, making it a dependable choice for weak labeling tasks.\n",
    "- **Good Generalization**: The similar performance on both validation and test sets suggests the model generalizes well to new, unseen data."
   ],
   "id": "de9eb62a9f670a74"
  },
  {
   "cell_type": "markdown",
   "id": "1f0a01bb",
   "metadata": {},
   "source": [
    "## Approach 2: KNeighbors Weak Labeling\n",
    "\n",
    "The KNeighbors classifier is chosen for its intuitive approach to classification by considering the proximity of data points in the feature space. This method is particularly suitable for weak labeling as it leverages the notion of clustering, which is analogous to finding the nearest neighbors for a given data point. By using Grid Search to tune hyperparameters, we aim to identify the optimal configuration that can enhance the performance of the weak labeling model."
   ]
  },
  {
   "cell_type": "code",
   "id": "0512b9ef",
   "metadata": {},
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(),\n",
    "                           param_grid,\n",
    "                           n_jobs=-1,\n",
    "                           cv=5,\n",
    "                           verbose=3)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "knn_model = grid_search.best_estimator_\n",
    "\n",
    "print_evaluation(y_val, knn_model.predict(X_val),\n",
    "                 'KNeighbors Weak Labeling (Validation Set)')\n",
    "\n",
    "print_evaluation(y_test, knn_model.predict(X_test),\n",
    "                 'KNeighbors Weak Labeling (Test Set)')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "630f5a664f440f56"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Class Detection**: Higher recall for class 1 (0.85) compared to class 0 (0.78-0.74) indicates the model is more effective at identifying positive instances, beneficial for tasks like fraud detection where identifying positives is crucial. As this is a weak labeling task, the model is expected to be better at detecting positive instances.\n",
    "- **Precision-Recall Trade-off**: Slightly higher precision for class 0 suggests the model may misclassify some positive instances as negative, a trade-off that needs careful management based on task requirements.\n",
    "- **Moderate Generalization**: Accuracy ranging from 0.82 to 0.79 shows reasonable performance but indicates a slight decline on the test set, suggesting the model could improve in handling unseen data.\n",
    "- **Dimensionality Challenges**: The variability in performance metrics could highlight potential issues with high-dimensional data, suggesting techniques like PCA could enhance performance by focusing on key features in a lower-dimensional space."
   ],
   "id": "3a60324c1c637efd"
  },
  {
   "cell_type": "markdown",
   "id": "0d6483e2",
   "metadata": {},
   "source": [
    "## Approach 2.1 KNeighbors Weak Labeling on PCA reduced embeddings\n",
    "\n",
    "To address the limitations of the KNeighbors classifier in high-dimensional spaces, we incorporate Principal Component Analysis (PCA) to reduce the dimensionality of the sentence embeddings. This step is justified by the fact that KNeighbors can struggle with high-dimensional data, leading to the curse of dimensionality. PCA helps in capturing the most significant features, thereby improving the efficiency and performance of the KNeighbors classifier in the weak labeling task.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "eed1000c",
   "metadata": {},
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=8)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "knn_pca_model = grid_search.best_estimator_\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "\n",
    "print_evaluation(y_val, knn_pca_model.predict(pca.transform(X_val)),\n",
    "                 'KNeighbors Weak Labeling (PCA Reduced) (Validation Set)')\n",
    "\n",
    "print_evaluation(y_test, knn_pca_model.predict(pca.transform(X_test)),\n",
    "                 'KNeighbors Weak Labeling (PCA Reduced) (Test Set)')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Dimensionality Reduction Impact**: Applying PCA to reduce dimensionality resulted in lower performance metrics compared to the original KNeighbors approach.\n",
    "- **Class Detection**: Recall for class 1 is 0.79, while for class 0 it is 0.70, indicating better detection of positive instances but weaker performance for negative instances.\n",
    "- **Overall Accuracy**: The accuracy of 0.74 suggests that while PCA helps in reducing the feature space, it might also lead to loss of important information, affecting the model's performance.\n",
    "- **Balanced Metrics**: Precision, recall, and F1-score are all 0.74, indicating that the model performs equally well across different metrics but at a lower level compared to the non-PCA approach."
   ],
   "id": "4420b3fcafcea0e"
  },
  {
   "cell_type": "markdown",
   "id": "d7f81b49",
   "metadata": {},
   "source": [
    "## Approach 3: Random Forest\n",
    "\n",
    "Random Forest is selected for its robustness and ability to handle complex data through ensemble learning. This approach justifies its use by combining multiple decision trees to improve classification accuracy and prevent overfitting. Random Forests are particularly effective in capturing nonlinear relationships and interactions between features, making them a powerful choice."
   ]
  },
  {
   "cell_type": "code",
   "id": "4c34640c",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "gradient_boosting_param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [3, 5, 7, 9, 11, 13]\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(RandomForestClassifier(),\n",
    "                              gradient_boosting_param_grid,\n",
    "                              n_jobs=-1,\n",
    "                              cv=5,\n",
    "                              verbose=3)\n",
    "\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "rf_model = rf_grid_search.best_estimator_\n",
    "print(f'Best Parameters: {rf_grid_search.best_params_}')\n",
    "\n",
    "print_evaluation(y_val, rf_model.predict(X_val),\n",
    "                 'Random Forest (Validation Set)')\n",
    "\n",
    "print_evaluation(y_test, rf_model.predict(X_test),\n",
    "                 'Random Forest (Test Set)')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Balanced Performance**: Precision, recall, and F1-score are consistently high at 0.83-0.84 for both classes, indicating balanced and accurate classification without favoring either class.\n",
    "- **High Reliability**: An accuracy of 0.83 across both validation and test sets suggests that the Random Forest model is reliable and makes correct predictions in a significant majority of cases.\n",
    "- **Good Generalization**: The similar performance metrics on both the validation and test sets indicate that the model generalizes well to new, unseen data, which is essential for real-world applications.\n",
    "- **Effective Ensemble Learning**: The consistent performance metrics reflect the robustness of Random Forest's ensemble learning approach, which combines multiple decision trees to improve classification accuracy and prevent overfitting."
   ],
   "id": "877215d877ebf043"
  },
  {
   "cell_type": "markdown",
   "id": "7d10beed",
   "metadata": {},
   "source": [
    "## Approach 4: Neural Network\n",
    "\n",
    "Neural Networks are chosen for their capability to model complex patterns and relationships in the data. This approach is justified by the neural network's ability to learn from high-dimensional sentence embeddings and possibly capture intricate feature representations."
   ]
  },
  {
   "cell_type": "code",
   "id": "2f8507f6",
   "metadata": {},
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_param_grid = {\n",
    "    'hidden_layer_sizes': [(1024, 512), (512, 128), (256, 64), (128, 32)],\n",
    "    'max_iter': [10000],\n",
    "}\n",
    "\n",
    "mlp_grid_search = GridSearchCV(MLPClassifier(),\n",
    "                               mlp_param_grid,\n",
    "                               n_jobs=-1,\n",
    "                               cv=5,\n",
    "                               verbose=3)\n",
    "mlp_grid_search.fit(X_train, y_train)\n",
    "mlp_model = mlp_grid_search.best_estimator_\n",
    "\n",
    "plt.plot(mlp_model.loss_curve_)\n",
    "plt.title('Neural Network Loss Curve')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "print(f'Best Parameters: {mlp_grid_search.best_params_}')\n",
    "\n",
    "print_evaluation(y_val, mlp_model.predict(X_val),\n",
    "                 'Neural Network (Validation Set)')\n",
    "\n",
    "print_evaluation(y_test, mlp_model.predict(X_test),\n",
    "                 'Neural Network (Test Set)')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Convergence**: The loss curve shows that the neural network converges to a stable loss value, indicating that the model has effectively learned the underlying patterns in the data but may suffer from overfitting looking solely at the loss curve.\n",
    "- **Balanced Performance**: Precision, recall, and F1-score are all consistently at 0.84-0.85 for both classes, indicating the neural network's ability to accurately classify both positive and negative instances without bias.\n",
    "- **High Accuracy**: An accuracy of 0.84 across both validation and test sets reflects the model's strong ability to make correct predictions, showcasing its reliability for weak labeling tasks.\n",
    "- **Good Generalization**: The consistent performance on both the validation and test sets suggests that the neural network generalizes well to unseen data, an essential trait for real-world applications.\n",
    "- **Effective Feature Representation**: The balanced and high performance metrics indicate that the neural network is effectively capturing complex patterns and relationships in the data, leveraging its architecture to provide robust predictions."
   ],
   "id": "67478e7cbbc38f"
  },
  {
   "cell_type": "markdown",
   "id": "885aa1cf",
   "metadata": {},
   "source": [
    "## Approach 5: Support Vector Machine\n",
    "\n",
    "Support Vector Machines (SVM) are included for their effectiveness in high-dimensional spaces and robustness against overfitting. SVMs are particularly suitable for weak labeling as they can find the optimal hyperplane that maximizes the margin between different classes. This approach is justified by the SVM's strong theoretical foundation and its ability to handle both linear and nonlinear classification tasks through different kernel functions, ensuring versatility."
   ]
  },
  {
   "cell_type": "code",
   "id": "807e5ddb",
   "metadata": {},
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_param_grid = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1, 3, 5, 10],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "svm_grid_search = GridSearchCV(SVC(),\n",
    "                               svm_param_grid,\n",
    "                               n_jobs=-1,\n",
    "                               cv=5,\n",
    "                               verbose=3)\n",
    "\n",
    "svm_grid_search.fit(X_train, y_train)\n",
    "svm_model = svm_grid_search.best_estimator_\n",
    "print(f'Best Parameters: {svm_grid_search.best_params_}')\n",
    "\n",
    "print_evaluation(y_val, svm_model.predict(X_val),\n",
    "                 'Support Vector Machine (Validation Set)')\n",
    "\n",
    "print_evaluation(y_test, svm_model.predict(X_test),\n",
    "                 'Support Vector Machine (Test Set)')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **High Precision and Recall**: Precision and recall are both consistently at 0.86 across both classes, indicating the SVM's effectiveness in correctly identifying both positive and negative instances without bias.\n",
    "- **Consistent F1-Score**: An F1-score of 0.86 for both classes shows that the model maintains a balance between precision and recall, ensuring reliable performance.\n",
    "- **High Accuracy**: An accuracy of 0.86 on both validation and test sets suggests that the SVM is highly reliable and capable of making correct predictions consistently.\n",
    "- **Good Generalization**: The similar performance metrics on both the validation and test sets imply that the SVM generalizes well to unseen data, making it a robust choice for real-world applications.\n",
    "- **Effective Margin Maximization**: The strong and balanced performance of the SVM reflects its ability to effectively find the optimal hyperplane that maximizes the margin between classes, ensuring robust classification."
   ],
   "id": "2ee94076c1521019"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Comparison of All Approaches\n",
    "\n",
    "#### Logistic Regression\n",
    "- **Performance**: Consistently high precision, recall, and F1-score of roughly 0.86 across both classes on validation and test sets.\n",
    "- **Accuracy**: Validation set: 0.86, Test set: 0.86\n",
    "- **Generalization**: Excellent, with similar metrics on validation and test sets.\n",
    "- **Key Strength**: Simplicity and computational efficiency with robust generalization.\n",
    "\n",
    "#### KNeighbors\n",
    "- **Performance**: Lower recall for class 0 (0.78/0.74) but higher for class 1 (0.85 on both sets). Precision varies between classes.\n",
    "- **Accuracy**: Validation set: 0.82, Test set: 0.79\n",
    "- **Generalization**: Moderate, with a slight decline on the test set.\n",
    "- **Key Strength**: Effective for detecting class 1 instances; most likely high-dimensional data is the limiting factor.\n",
    "\n",
    "##### KNeighbors with PCA\n",
    "- **Performance**: Reduced precision and recall compared to standard KNeighbors, with balanced but lower metrics.\n",
    "- **Accuracy**: Validation set: 0.77, Test set: 0.74\n",
    "- **Generalization**: Weaker compared to other models and especially standard KNeighbors.\n",
    "- **Key Strength**: Dimensionality reduction; however, potential loss of important information.\n",
    "\n",
    "#### Random Forest\n",
    "- **Performance**: Balanced precision, recall, and F1-score of 0.83-0.84 across both classes.\n",
    "- **Accuracy**: Validation set: 0.83, Test set: 0.83\n",
    "- **Generalization**: Good, with consistent metrics on validation and test sets.\n",
    "- **Key Strength**: Robust ensemble learning, capturing complex relationships in the data.\n",
    "\n",
    "#### Neural Network\n",
    "- **Performance**: Balanced precision, recall, and F1-score of 0.84-0.85 across both classes on both sets.\n",
    "- **Accuracy**: Validation set: 0.84, Test set: 0.84\n",
    "- **Generalization**: Good, with consistent metrics on validation and test sets.\n",
    "- **Key Strength**: Effective feature representation and capturing complex patterns.\n",
    "\n",
    "#### Support Vector Machine (SVM)\n",
    "- **Performance**: Consistently high precision, recall, and F1-score of 0.86 across both classes.\n",
    "- **Accuracy**: Validation set: 0.86, Test set: 0.86\n",
    "- **Generalization**: Excellent, with similar metrics on validation and test sets.\n",
    "- **Key Strength**: Effective margin maximization, providing robust classification.\n",
    "\n",
    "### Summary\n",
    "- **Best Performers**: Logistic Regression and SVM show the highest and most consistent performance metrics, with an accuracy of 0.86 on both the validation and test set, indicating strong generalization and reliability.\n",
    "- **Good Performers**: Neural Network and Random Forest also demonstrate high performance and good generalization with slightly lower accuracy in the area around 0.83-0.84.\n",
    "- **Moderate Performer**: KNeighbors, both standard and with PCA, show reasonable performance but with lower accuracy and generalization compared to other models. The PCA approach particularly suffers from potential information loss, leading to weaker performance.\n",
    "\n",
    "Overall all weak labeling models show good performance with high accuracy and balanced metrics, indicating their reliability and effectiveness in classifying the data. To consider are the computational efficiency and generalization capabilities, with Logistic Regression and SVM standing out as the top performers in this regard."
   ],
   "id": "d0fe2c2366c7c2eb"
  },
  {
   "cell_type": "markdown",
   "id": "d754f632",
   "metadata": {},
   "source": [
    "## Save Best Models\n",
    "\n",
    "For the semi-supervised learning phase of the weak labeling, we will save the best weak labeling models. The saved models can be used for inference in the `weak_labelling.py` script. For an example usage of the weak labeling pipeline, refer to the `README.md` file.\n",
    "\n",
    "Although we will not all the models in the final pipeline, we will save them for reference and potential future use and completeness."
   ]
  },
  {
   "cell_type": "code",
   "id": "1d946e79",
   "metadata": {},
   "source": [
    "MODELS_FOLDER = os.getenv('MODELS_DIR', 'models')\n",
    "os.makedirs(MODELS_FOLDER, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_model(model, filename, overwrite=False):\n",
    "    if os.path.exists(f'{MODELS_FOLDER}/weak_labelling/{filename}.pkl') and not overwrite:\n",
    "        print(f'{filename} already exists. Skipping...')\n",
    "        return\n",
    "\n",
    "    with open(f'{MODELS_FOLDER}/weak_labelling/{filename}.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "\n",
    "save_model(log_reg_model, 'log_reg')\n",
    "save_model(knn_model, 'knn')\n",
    "save_model(knn_pca_model, 'knn_pca')\n",
    "save_model(rf_model, 'rf')\n",
    "save_model(mlp_model, 'mlp')\n",
    "save_model(svm_model, 'svm')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
