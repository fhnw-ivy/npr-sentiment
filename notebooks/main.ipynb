{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from src.data_loader import LABEL_MAP\n",
    "from src.data_loader import load_datasets\n",
    "from src.model_pipeline import load_and_prep_datasets"
   ],
   "id": "b3a5d027d29635",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "55bc471f",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Mini-Challenge\n",
    "\n",
    "> Authors: Dominik Filliger, Nils Fahrni, Noah Leuenberger (2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4b2dd",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Sentiment analysis is a crucial task in Natural Language Processing (NLP) that involves determining the sentiment or tone of a given text. It has numerous applications, such as understanding customer feedback, monitoring social media sentiment, and analyzing product reviews. However, manually labeling large datasets for sentiment analysis can be time-consuming and costly. Semi-supervised learning techniques, such as weak supervision, can help alleviate this challenge by leveraging a small amount of labeled data along with a larger set of unlabeled data to improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75667560",
   "metadata": {},
   "source": [
    " ## 2. Dataset Selection and Exploratory Data Analysis\n",
    "The dataset used for this mini-challenge is the Amazon Polarity dataset, which consists of product reviews from Amazon labeled as either positive or negative. The dataset is loaded using the Hugging Face Datasets library. Exploratory data analysis is performed to gain insights into the distribution of labels, length of reviews, and other relevant characteristics.\n",
    "\n",
    "As the dataset contains 4 million reviews we cut this down into a subset of 6666 reviews for the purpose of this mini-challenge. 666 of the reviews are used for validation, the remaining 6000 are split into 1000 labeled samples and 5000 artificially unlabeled samples.\n",
    "\n",
    "Each subset has a 50/50 split of positive and negative reviews. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_df, unlabeled, validation = load_datasets(\"../data/partitions\")\n",
    "\n",
    "print(f\"Labeled Dataset Length: {len(train_df)}\")\n",
    "print(f\"Unlabeled Dataset Length: {len(unlabeled)}\")\n",
    "print(f\"Validation Dataset Length: {len(validation)}\")"
   ],
   "id": "b7d121e24f626f5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To get a better idea of the whole dataset, we will merge the data again and perform some exploratory data analysis. For this purpose, we will merge the training, unlabeled, and validation datasets into one dataframe. We will also rename the 'ground_truth' column to 'label' in the unlabeled dataset to maintain consistency across the datasets.",
   "id": "e2a464f170d2351f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unlabeled.rename(columns={'ground_truth': 'label'}, inplace=True)  # Rename column for consistency\n",
    "eda_df = pd.concat([train_df, unlabeled, validation])\n",
    "eda_df['label'] = eda_df['label'].map(LABEL_MAP)  # Map labels to 0: Negative, 1: Positive\n",
    "\n",
    "print(f\"Merged Dataset Length: {len(eda_df)}\")\n",
    "eda_df.head()"
   ],
   "id": "bd69ceb3469b9745",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "eda_df['review_length'] = eda_df['content'].apply(len)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(eda_df['review_length'], bins=50, kde=True)\n",
    "plt.title('Distribution of Review Lengths in Training Data')\n",
    "plt.xlabel('Review Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "id": "5b0b9ca062123f99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The distribution of review lengths in the dataset is visualized using a histogram. The majority of reviews have a length between 0 and 1000 characters, with a peak around 500 characters. This information can be useful for preprocessing and feature engineering steps in the sentiment analysis task.",
   "id": "8cb051063ad5b7f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Most Common Words",
   "id": "5a1f960b012c2b6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_most_common_words(df, top_n=20):\n",
    "    df = df.copy()\n",
    "    df['label'] = df['label'].map({v: k for k, v in LABEL_MAP.items()})\n",
    "    pos_reviews = df[df['label'] == 1]['content']\n",
    "    neg_reviews = df[df['label'] == 0]['content']\n",
    "\n",
    "    vectorizer_pos = CountVectorizer(stop_words='english')\n",
    "    vectorizer_neg = CountVectorizer(stop_words='english')\n",
    "\n",
    "    pos_word_count = vectorizer_pos.fit_transform(pos_reviews)\n",
    "    neg_word_count = vectorizer_neg.fit_transform(neg_reviews)\n",
    "\n",
    "    pos_sum_words = pos_word_count.sum(axis=0)\n",
    "    neg_sum_words = neg_word_count.sum(axis=0)\n",
    "\n",
    "    pos_words_freq = [(word, pos_sum_words[0, idx]) for word, idx in\n",
    "                      zip(vectorizer_pos.get_feature_names_out(), range(pos_sum_words.shape[1]))]\n",
    "    neg_words_freq = [(word, neg_sum_words[0, idx]) for word, idx in\n",
    "                      zip(vectorizer_neg.get_feature_names_out(), range(neg_sum_words.shape[1]))]\n",
    "\n",
    "    pos_words_freq = sorted(pos_words_freq, key=lambda x: x[1], reverse=True)\n",
    "    neg_words_freq = sorted(neg_words_freq, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    words, freq = zip(*pos_words_freq[:top_n])\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(words, freq)\n",
    "    plt.title('Most common words in positive reviews')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "    words, freq = zip(*neg_words_freq[:top_n])\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(words, freq)\n",
    "    plt.title('Most common words in negative reviews')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_most_common_words(eda_df, top_n=20)"
   ],
   "id": "8b944f080f06f4b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The most common words in positive and negative reviews are visualized using bar plots. The top 20 most frequent words in each category are displayed, providing insights into the language used in positive and negative reviews.",
   "id": "2f82857c221e4833"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Word Clouds",
   "id": "c818f28f4e8bde81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_word_cloud(text, title):\n",
    "    wordcloud = WordCloud(width=800, height=800,\n",
    "                          background_color='white',\n",
    "                          stopwords=set(STOPWORDS),\n",
    "                          min_font_size=10).generate(text)\n",
    "\n",
    "    plt.figure(figsize=(8, 8), facecolor=None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ],
   "id": "eda21d9741f19da5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pos_reviews_text = \" \".join(eda_df[eda_df['label'] == \"positive\"]['content'].values)\n",
    "generate_word_cloud(pos_reviews_text, \"Word Cloud for Positive Reviews\")"
   ],
   "id": "b6ab0019c556ed8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "neg_reviews_text = \" \".join(eda_df[eda_df['label'] == \"negative\"]['content'].values)\n",
    "generate_word_cloud(neg_reviews_text, \"Word Cloud for Negative Reviews\")"
   ],
   "id": "3aeefd9b49d6905",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6b901248",
   "metadata": {},
   "source": [
    "## 3. Data Splitting Strategy\n",
    "The dataset is split into development, validation, labeled, and unlabeled sets using a nested split approach. The development set is a fraction of the full dataset, the validation set is a fraction of the test dataset, and the labeled set is a fraction of the development set. The remaining samples in the development set are considered unlabeled. The nested split always adds in 25% increments (25, 50, 75), and a 1/6 split between labelled and unlabelled data is used, resulting in 1000 labeled and 5000 weakly labeled samples in total.\n",
    "\n",
    "All the pre-split datasets are stored in the `data/partitions` directory as `.parquet` files.\n",
    "\n",
    "Given the focus of the MC on the impact of weak labelling and its impact, we introduce a nested split which further divides our training data into splits. Here is a brief overview of the nested split algorithm we use:\n",
    "\n",
    "1. **Validate the Fractions**: We start by ensuring that the proportions we want to use for our subsets are reasonableâ€”each should be a fraction of the whole dataset.\n",
    "2. **Shuffle the Data**: To make sure our subsets are representative and unbiased, we randomly shuffle the entire dataset. This ensures that each subset is a good mix of the data.\n",
    "3. **Forming the Subsets**: For each proportion we decided on, we calculate how much of the dataset it represents. Starting with the smallest subset, we keep adding more data until we reach the desired size for each proportion. Each new subset contains all the data from the previous subsets, plus some more.\n",
    "4. **Collect the Subsets**: In the end, we have a series of nested subsets, each larger than the last.\n",
    "\n",
    "The implementation of the nested split is used in the `load_and_prep_datasets` function in the `src/model_pipeline.py` module and separately implemented in the `src/prep_datasets.py` script.\n",
    "\n",
    "As the goal is to identify the optimal amount of additional data that can be used to improve model performance without the need for manual annotation. For this purpose: When training with weak labels we only apply the nested split on the weak labels and not the labeled data and then concat every given nested split with all the labeled data."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(10, 5), sharey=True)\n",
    "\n",
    "\n",
    "def add_percentages(ax, data):\n",
    "    total = len(data)\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        percentage = f'{(height / total) * 100:.1f}%'\n",
    "        ax.text(p.get_x() + p.get_width() / 2., height, percentage, ha='center', va='bottom')\n",
    "\n",
    "\n",
    "sns.countplot(data=train_df, x='label', ax=ax[0])\n",
    "add_percentages(ax[0], train_df)\n",
    "ax[0].set_title('Labeled Dataset')\n",
    "ax[0].set_xlabel('Label')\n",
    "ax[0].set_ylabel('Count')\n",
    "\n",
    "sns.countplot(data=unlabeled, x='label', ax=ax[1])\n",
    "add_percentages(ax[1], unlabeled)\n",
    "ax[1].set_title('Unlabeled Dataset')\n",
    "ax[1].set_xlabel('Ground Truth')\n",
    "\n",
    "sns.countplot(data=validation, x='label', ax=ax[2])\n",
    "add_percentages(ax[2], validation)\n",
    "ax[2].set_title('Validation Dataset')\n",
    "ax[2].set_xlabel('Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "73e761cf1be82629",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The distribution of labels in the labeled, unlabeled, and validation datasets is visualized using count plots. The percentage of positive and negative reviews in each dataset is displayed, providing insights into the class distribution of the data. As we can see, the datasets are balanced with a 50/50 split between positive and negative reviews making imbalanced classes no issue for our models.",
   "id": "1208a1b815c2df7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "preped_datasets = load_and_prep_datasets(\"../data\", nested_splits=True)\n",
    "logging.disable(\n",
    "    logging.CRITICAL)  # Disable logging for this cell as logging is initialized in the load_and_prep_datasets function\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "sns.barplot(x=list(preped_datasets['nested_splits'].keys()),\n",
    "            y=[len(value) for value in preped_datasets['nested_splits'].values()], ax=ax)\n",
    "ax.set_title('Nested Split Sizes')\n",
    "ax.set_xlabel('Nested Split Fraction')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "total = len(preped_datasets['train'])\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    percentage = f'{(height / total) * 100:.1f}%'\n",
    "    ax.text(p.get_x() + p.get_width() / 2., height, percentage, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ],
   "id": "d2e07a60e048ddfe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The sizes of the nested splits generated from the training data are visualized using a bar plot. The count of samples in each nested split is displayed, along with the percentage of the total training data that each split represents. The nested splits are created in 25% increments, starting from 25% of the training data and increasing to 100% of the training data. In this visualization the nested split was only applied to the labeled data but the picture would be the same if it was applied to the weak labels.",
   "id": "91b113205d7242d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(10, 5), sharey=True)\n",
    "\n",
    "for i, (key, value) in enumerate(preped_datasets['nested_splits'].items()):\n",
    "    value = value.to_pandas()\n",
    "    sns.countplot(data=value, x='label', ax=ax[i])\n",
    "    add_percentages(ax[i], value)\n",
    "    ax[i].set_title(f'Nested Split {key}')\n",
    "    ax[i].set_xlabel('Label')\n",
    "    ax[i].set_ylabel('Count')"
   ],
   "id": "88e3250115fcb9a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The distribution of labels in each nested split is visualized using count plots. The percentage of positive and negative reviews in each nested split is displayed, providing insights into the class distribution of the data. The nested splits maintain a balanced distribution of positive and negative reviews, as the set the data is sampled from is also balanced.",
   "id": "2388c008fca6b305"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Helper Functions\n",
    "In order to make the notebook more readable and to avoid code duplication, we will define some helper functions that will be used throughout the notebook. These functions will help us to perform common tasks such as running experiments, evaluating pipelines, and visualizing the results.\n"
   ],
   "id": "4c69744358a10150"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_DIR = os.getenv(\"MODELS_DIR\")\n",
    "\n",
    "def plot_model_performance(results_data, model_names, baseline_data=None, metrics=None, baseline_name='Baseline'):\n",
    "    if metrics is None:\n",
    "        metrics = ['eval_accuracy', 'eval_f1_macro', 'eval_f1_weighted']\n",
    "\n",
    "    results = results_data\n",
    "\n",
    "    fractions = sorted(results[0].keys(), key=float)\n",
    "    num_fractions = len(fractions)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    x = range(num_fractions)\n",
    "\n",
    "    if baseline_data:\n",
    "        baseline_accuracy = baseline_data['eval_accuracy']\n",
    "        ax.axhline(y=baseline_accuracy, color='r', linestyle='--', label=baseline_name)\n",
    "\n",
    "    for i, model_results in enumerate(results):\n",
    "        values = [model_results[fraction]['eval_accuracy'] for fraction in fractions]\n",
    "        ax.plot(x, values, marker='o', label=model_names[i])\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(fractions)\n",
    "    ax.set_xlabel('Fraction of Labeled Samples')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Model Accuracy Comparison')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_model_auroc(results_data, model_names, baseline_data=None, baseline_name='Baseline'):\n",
    "    results = results_data\n",
    "\n",
    "    fractions = sorted(results[0].keys(), key=float)\n",
    "    num_fractions = len(fractions)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    x = range(num_fractions)\n",
    "\n",
    "    if baseline_data:\n",
    "        baseline_auroc = baseline_data['eval_auroc']\n",
    "        ax.axhline(y=baseline_auroc, color='r', linestyle='--', label=baseline_name)\n",
    "\n",
    "    for i, model_results in enumerate(results):\n",
    "        values = [model_results[fraction]['eval_auroc'] for fraction in fractions]\n",
    "        ax.plot(x, values, marker='o', label=model_names[i])\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(fractions)\n",
    "    ax.set_xlabel('Fraction of Labeled Samples')\n",
    "    ax.set_ylabel('AUROC')\n",
    "    ax.set_title('Model AUROC Comparison')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "51c8c9ffc108cafe",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "47939c9e",
   "metadata": {},
   "source": [
    "## 4. Baseline Model Performance\n",
    "The selected model is a pretrained language model, specifically `sentence-transformers/all-MiniLM-L6-v2`, which is used as the baseline model for sentiment classification without any training. \n",
    "   \n",
    "Before we train any models, we will evaluate the performance of the baseline model on the validation set. The model will be used to predict the sentiment of the reviews in the validation set, and the results will be evaluated using accuracy, precision, recall, and F1-score metrics.\n",
    "\n",
    "**DISCLAIMER**: In this notebook no training or evaluating is done it. It only loads the results from the training and evaluation done via the `src/model_pipeline.py` module. For more info check out the README file."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "relevant_metrics = ['eval_accuracy', 'eval_f1_macro', 'eval_f1_weighted']\n",
    "\n",
    "with open(f'{MODEL_DIR}/eval/eval_results.json') as file:\n",
    "    baseline_data = json.load(file)\n",
    "\n",
    "metrics = [metric for metric in relevant_metrics if metric in baseline_data]\n",
    "values = [baseline_data[metric] for metric in metrics]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.bar(metrics, values)\n",
    "ax.set_title('Baseline Model Performance')\n",
    "ax.set_xlabel('Metric')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "bb60a816f1489469",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The results speak for themselves. The model without any training achieves very poor results. With around 50% accuracy, it is only slightly better than random guessing.\n",
    "\n",
    "This sets the perfect foundation for our experiments. \n"
   ],
   "id": "8657b7ae8fe0969d"
  },
  {
   "cell_type": "markdown",
   "id": "a8ef2289",
   "metadata": {},
   "source": [
    "## 5. Supervised Learning Performance\n",
    "\n",
    "Before we dive deeper into the chosen weak labelling technique and its impact on the model performance, we will first decide whether we will train our model via transfer learning or fine-tuning.\n",
    "\n",
    "For this we will train the model using the nested splits on both techniques and compare the results. The results are stored in the `data/eval` directory as `.json` files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9317a134",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1 Transfer Learning"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(f'{MODEL_DIR}/supervised/transfer_nested/eval_results.json') as file:\n",
    "    transfer_nested_data = json.load(file)\n",
    "\n",
    "plot_model_performance([transfer_nested_data], ['Transfer Learning'], baseline_data, metrics=relevant_metrics)\n",
    "\n"
   ],
   "id": "3bd5576049fc6c83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The results show that the transfer learning model barely outperforms the baseline model. This indicates that the pretrained model's knowledge is not sufficient to achieve high performance on the sentiment analysis task. ",
   "id": "5d0cb8c4c5b514f2"
  },
  {
   "cell_type": "markdown",
   "id": "23d7a8e1",
   "metadata": {},
   "source": [
    "### 5.2 Fine-tuning\n",
    "To identify if fine-tuning the model can improve the performance, we will train the model using the nested splits and compare the results. \n",
    "\n",
    "For the fine-tuning we are using the recommended hyperparameters from the Hugging Face documentation. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(f'{MODEL_DIR}/supervised/finetune_nested/eval_results.json') as file:\n",
    "    finetune_nested_data = json.load(file)\n",
    "\n",
    "plot_model_performance([finetune_nested_data], ['Fine-tuning'], baseline_data, metrics=relevant_metrics)\n"
   ],
   "id": "1871656e5dc97ef9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The results show that fine-tuning outperforms both the baseline model and the transfer learning model. We can also see that after using 75% of the labelled data (750 labelled samples) the model stagnates in its performance. This indicates that the model has reached its capacity to learn from the data and adding more data does not substantially improve the performance.\n",
   "id": "d760fdb577139928"
  },
  {
   "cell_type": "markdown",
   "id": "22cfcf6e",
   "metadata": {},
   "source": [
    "## 6. Semi-Supervised Learning Performance\n",
    "After we established that fine-tuning is the best approach for training the model, we will now evaluate the performance of the semi-supervised learning techniques. We will compare the performance of the fine-tuned model with weak labels generated using different weak labelling strategies. \n",
    "\n",
    "The nested split logic above is used, with the small difference that each split contains the fully labeled data. This means that the nested split is applied to the weak labels and then concatenated with the fully labeled data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c53e734",
   "metadata": {},
   "source": "### 6.1 Logistic Regression (LogReg) Weak Labelling"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(f'{MODEL_DIR}/semi-supervised/finetune_nested/eval_results.json') as file:\n",
    "    logreg_nested_data = json.load(file)\n",
    "    \n",
    "plot_model_performance([logreg_nested_data], ['LogReg Weak-Labelling'], finetune_nested_data[\"1.0\"], metrics=relevant_metrics, baseline_name='Fine-tuning 100% (Fully Labeled)')\n"
   ],
   "id": "6d5220fe8adfcba9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Adding weak labels to the dataset has a significant impact on the model performance. With only an addition 25% ",
   "id": "79dc94ed9017120e"
  },
  {
   "cell_type": "markdown",
   "id": "e3d33785",
   "metadata": {},
   "source": "## 7. Learning Curve Analysis"
  },
  {
   "cell_type": "code",
   "id": "eabcf731",
   "metadata": {},
   "source": [
    "# Plot all results\n",
    "plot_model_performance([transfer_nested_data, finetune_nested_data, logreg_nested_data], ['Transfer Learning', 'Fine-tuning', 'LogReg Weak-Labelling'], baseline_data, metrics=relevant_metrics)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using the ",
   "id": "ea3d85c7caa6c502"
  },
  {
   "cell_type": "markdown",
   "id": "7ce2752d",
   "metadata": {},
   "source": [
    "## 8. Model Comparison and Analysis\n",
    "A thorough analysis of the results is conducted, comparing the baseline model, supervised learning techniques, and semi-supervised learning techniques. The impact of different weak labeling strategies and training data sizes on model performance is evaluated. The best approach for the chosen dataset is determined, emphasizing the models that achieve acceptable performance with few manually annotated samples."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot all results\n",
    "plot_model_auroc([transfer_nested_data, finetune_nested_data, logreg_nested_data], ['Transfer Learning', 'Fine-tuning', 'LogReg Weak-Labelling'], baseline_data)"
   ],
   "id": "76592bb8196721ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "45316041",
   "metadata": {},
   "source": [
    "## 9. Time Savings Factor and Implications\n",
    "The time savings factor, quantifying the reduction in manually labeled data required to achieve acceptable performance levels using weak labeling approaches, is calculated. The implications of the findings are discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fd172b",
   "metadata": {},
   "source": [
    "## 10. Conclusion and Future Directions\n",
    "The key findings, insights, and potential implications of the sentiment analysis mini-challenge are summarized. The effectiveness of weak supervision techniques in reducing the need for manual annotation while maintaining acceptable model performance is discussed. Future directions for research and improvements are outlined."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
