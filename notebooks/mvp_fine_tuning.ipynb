{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8debd106f5c41475",
   "metadata": {},
   "source": [
    "# MVP: Fine-tuning a Pretrained Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae172404e4ebc86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from src.data_loader import load_datasets\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "SEED = 1337\n",
    "\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "def set_seed():\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a1a84cfffc98e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_dev, unlabelled_dev, val_set, nested_splits = load_datasets(\"../data/\", include_nested_splits=True)\n",
    "train_df = pd.concat([labelled_dev, unlabelled_dev])\n",
    "\n",
    "labelled_dev.shape, unlabelled_dev.shape, val_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710ad5bc7bd400c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in nested_splits.items():\n",
    "    print(split[0], split[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0bff73deaa9e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94be54705673da4",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f60c9046bf3b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, force_download=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, force_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f58188031f124",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f\"Model has {params} trainable parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bafa2b410e1c6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb1e24d3ad77bb3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_epochs = 3\n",
    "training_batch_size = 16\n",
    "logging_steps = len(train_df) // training_batch_size\n",
    "\n",
    "# TODO: Use hyperparams for fine-tuning stated on https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
    "training_args = TrainingArguments(output_dir=MODEL_NAME,\n",
    "                                  num_train_epochs=training_epochs,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=training_batch_size,\n",
    "                                  per_device_eval_batch_size=training_batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  log_level=\"error\",\n",
    "                                  report_to=\"wandb\",\n",
    "                                  run_name=\"amazon_sentiment_analysis\",\n",
    "                                  optim=\"adamw_torch\"\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d53bab06d15ad5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(batch, max_length=512):\n",
    "    return tokenizer(batch['content'], padding=True, truncation=True, max_length=max_length)\n",
    "\n",
    "def tokenize_dataset(dataset, max_size=100, process_batch_size=100, batched=True):\n",
    "    \"\"\" Tokenizes the dataset \"\"\"\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        raise ValueError(\"The dataset must be a huggingface Dataset object.\")\n",
    "    return dataset.map(lambda batch: tokenize(batch, max_size), batched=batched, batch_size=process_batch_size)\n",
    "\n",
    "train_ds_tokenized = tokenize_dataset(train_ds)\n",
    "val_ds_tokenized = tokenize_dataset(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa7923a0b44e1f5",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a9a859d036d14",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels, preds = pred.label_ids, pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5155e78be1c3b9ef",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainerCallback\n",
    "\n",
    "class EpochResultsCallback(TrainerCallback):\n",
    "    \"\"\"A custom callback to capture and log results at the end of each epoch.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.results_df = pd.DataFrame(columns=['Epoch', 'Validation Loss', 'Accuracy', 'F1'])\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        new_row = {\n",
    "            'Epoch': state.epoch,\n",
    "            'Validation Loss': metrics['eval_loss'],\n",
    "            'Accuracy': metrics.get('eval_accuracy', None),\n",
    "            'F1': metrics.get('eval_f1', None)\n",
    "        }\n",
    "        self.results_df = pd.concat([self.results_df, pd.DataFrame(new_row, index=[0])])\n",
    "\n",
    "\n",
    "def fine_tune_model(model, training_args, train_dataset, eval_dataset, tokenizer):\n",
    "    epoch_results_callback = EpochResultsCallback()\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[epoch_results_callback]\n",
    "    )\n",
    "    \n",
    "    trainer.evaluate()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    trainer.train()\n",
    "\n",
    "    return epoch_results_callback.results_df\n",
    "\n",
    "eval_df = fine_tune_model(model, training_args, train_ds_tokenized, val_ds_tokenized, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f1e7bfda45b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb8517b16eb71b5",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb7f21c8e0fc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
