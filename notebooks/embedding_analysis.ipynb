{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook at hand aims to dive into the possible patterns dimensionality reduction techniques can show within the proposed weak labeling models and the embedding models used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Weak Labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import plotly.express as px\n",
    "import plotly\n",
    "import pandas as pd\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "plotly.offline.init_notebook_mode()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WL_DATA_DIR = os.getenv('DATA_DIR', 'data')\n",
    "WL_DATA_DIR = os.path.abspath(os.path.join(parent_dir, WL_DATA_DIR, 'weak_labelled'))\n",
    "\n",
    "weak_labelled = {}\n",
    "\n",
    "print(f\"Reading weak labelled data from {WL_DATA_DIR}\")\n",
    "for file in os.listdir(WL_DATA_DIR):\n",
    "    weak_labelled[file] = pd.read_parquet(os.path.join(WL_DATA_DIR, file))\n",
    "    print(f\"- Read {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Labeled data\n",
    "To project the high dimensional embeddings into a humanly readable format we implemented Arize's Phoenix app that allows us to interactively look at the embedding space projected down into 3 dimensions by UMAP.\n",
    "\n",
    "Additionally, it might be insightful to also look at a different dimension reduction approach - Therefore we made the `plot_pca` function which will project the embedding space into two dimensions using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def break_content(text, length=50):\n",
    "        lines = []\n",
    "        while len(text) > length:\n",
    "            space_index = text.rfind(' ', 0, length)\n",
    "            if space_index == -1:\n",
    "                space_index = length\n",
    "            lines.append(text[:space_index])\n",
    "            text = text[space_index:].lstrip()\n",
    "        lines.append(text)\n",
    "        return '<br>'.join(lines)\n",
    "\n",
    "def plot_pca(weak_labelled, key):\n",
    "    if key not in weak_labelled:\n",
    "        raise ValueError(f\"File {key} not found in the weak_labelled dictionary.\")\n",
    "    \n",
    "    df = weak_labelled[key]\n",
    "\n",
    "    embeddings = np.vstack(df['embedding_vec'].values)\n",
    "    labels = np.array(df['label'].values)\n",
    "    content = df['content'].apply(lambda x: break_content(x)).values\n",
    "\n",
    "    pca = PCA(n_components=3)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "    \n",
    "    pca_df = pd.DataFrame(reduced_embeddings, columns=['PCA1', 'PCA2', 'PCA3'])\n",
    "    pca_df['Label'] = labels\n",
    "    pca_df['Content'] = content\n",
    "\n",
    "    fig = px.scatter_3d(pca_df, x='PCA1', y='PCA2', z='PCA3', color='Label', \n",
    "                        title=f'PCA of Embedding Vectors for {key}',\n",
    "                        size_max=5, opacity=0.6, height=800,\n",
    "                        hover_data={'Content': True})\n",
    "    fig.update_traces(marker=dict(size=2))\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Weak Labels\n",
    "For this first view onto the embedding space we will look at how the KNN labels the sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.px_utils import create_dataset, launch_px\n",
    "\n",
    "knn_key = 'mlp_weak_labeling_weaklabels.parquet'\n",
    "\n",
    "knn_wl_ds = create_dataset('knn', weak_labelled[knn_key], \n",
    "                           weak_labelled[knn_key]['embedding_vec'], \n",
    "                           weak_labelled[knn_key]['label'])\n",
    "\n",
    "px_session = launch_px(knn_wl_ds, None)\n",
    "px_session.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster distancing itself the farthest seems to consist almost solely of **music album reviews** that were labeled with `1`, meaning a positive sentiment.\n",
    "\n",
    "Otherwise, the labels don't show a specific pattern or clustering. However, the embedding positions in the space are relatively clearly clustered: We are using the embedding vectors of huggingface's `all-MiniLM-L6-v2` BERT sentence transformer. This sentence transformer was trained on sentence pairs that appear as a Q&A. The resulting vector embedding therefore describes the semantic content of such a sentence - This is exactly what we can see in the embedding space; Reviews of the `amazon-polarity` dataset are clustered together according to their product niche, as for example already mentioned with the cluster containing music reviews.\n",
    "\n",
    "But we can also observe other semantic relationships:\n",
    "- Video game reviews lie between music and book reviews: This axis could perhaps describe interactivity; music can be enjoyed passively, games do have some interactions between cutscenes while books capture ones concentration and attention entirely.\n",
    "- Video game reviews lie opposite of tech gadgets and other devices: This axis might describe the abstraction of virtuality. Games are completely virtual tech while tech gadgets are physical devices.\n",
    "- Kid's toys are clustered between games and tech gadgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Phoenix doesn't allow for a different dimension reduction technique we implement a PCA strategy ourselves. The UMAP technique differs vastly from PCA so looking at another technique could yield more interesting observations in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(weak_labelled, knn_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the UMAP representation the PCA reduction doesn't seem to show much more separation in labels or semantics. We can still roughly see the following four clusters:\n",
    "- Music albums\n",
    "- Books\n",
    "- Movies\n",
    "- Tech Gadgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a sentence transformer with higher dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_key = 'log_reg_weak_labeling_weaklabels.parquet'\n",
    "\n",
    "log_reg_wl_ds = create_dataset('log_reg', weak_labelled[log_reg_key], \n",
    "                           weak_labelled[log_reg_key]['embedding_vec'], \n",
    "                           weak_labelled[log_reg_key]['label'])\n",
    "\n",
    "px_session = launch_px(log_reg_wl_ds, None)\n",
    "px_session.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(weak_labelled, log_reg_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron Weak Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_key = 'mlp_weak_labeling_weaklabels.parquet'\n",
    "\n",
    "mlp_wl_ds = create_dataset('mlp_reg', weak_labelled[mlp_key], \n",
    "                           weak_labelled[mlp_key]['embedding_vec'], \n",
    "                           weak_labelled[mlp_key]['label'])\n",
    "\n",
    "px_session = launch_px(mlp_wl_ds, None)\n",
    "px_session.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(weak_labelled, mlp_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Weak Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_key = 'rf_weak_labeling_weaklabels.parquet'\n",
    "\n",
    "rf_wl_ds = create_dataset('rf_reg', weak_labelled[rf_key], \n",
    "                           weak_labelled[rf_key]['embedding_vec'], \n",
    "                           weak_labelled[rf_key]['label'])\n",
    "\n",
    "px_session = launch_px(rf_wl_ds, None)\n",
    "px_session.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(weak_labelled, rf_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Weak Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_key = 'svm_weak_labeling_weaklabels.parquet'\n",
    "\n",
    "svm_wl_ds = create_dataset('svm_reg', weak_labelled[svm_key], \n",
    "                           weak_labelled[svm_key]['embedding_vec'], \n",
    "                           weak_labelled[svm_key]['label'])\n",
    "\n",
    "px_session = launch_px(svm_wl_ds, None)\n",
    "px_session.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(weak_labelled, svm_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
