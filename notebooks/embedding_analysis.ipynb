{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook at hand aims to dive into the possible patterns dimensionality reduction techniques can show within the proposed embedding models.\n",
    "\n",
    "To analyze the embedding spaces we use Arize's Phoenix app which decomposes the high dimensionality into a 3-dimensional space using UMAP. Additionally we will also look at each embedding space in a PCA-decomposed representation to see how much impact the decomposition algorithm may have.\n",
    "\n",
    "We looked at the performance of the following two embedding models:\n",
    "- [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n",
    "- [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "plotly.offline.init_notebook_mode()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Embeddings\n",
    "The first step is to load the persisted embeddings from each embedding model. The embedding vectors (i.e. embedding vector matrix) was saved in the `data/embeddings/*` folder for each split in the initial train, test and validation split. \n",
    "\n",
    "The following block loads these split up matrices into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.getenv('DATA_DIR', 'data')\n",
    "EMBEDDING_DATA_DIR = os.path.abspath(os.path.join(parent_dir, DATA_DIR, 'embeddings'))\n",
    "\n",
    "weak_labelled = {}\n",
    "\n",
    "print(f\"Reading weak labelled data from {EMBEDDING_DATA_DIR}\")\n",
    "\n",
    "embedding_model_dirs = [d for d in os.listdir(EMBEDDING_DATA_DIR) if os.path.isdir(os.path.join(EMBEDDING_DATA_DIR, d))]\n",
    "embeddings = {}\n",
    "\n",
    "for dir in embedding_model_dirs:\n",
    "    print(f\"- Opening Embeddings from {dir}\")\n",
    "    curr_embeddings = {}\n",
    "    for file in os.listdir(os.path.join(EMBEDDING_DATA_DIR, dir)):\n",
    "        if file.endswith('.pkl'):\n",
    "            filename = file.split('.')[0]\n",
    "            curr_embeddings[filename] = pd.read_pickle(os.path.join(EMBEDDING_DATA_DIR, dir, file))\n",
    "        print(f\"  - Read {file}\")\n",
    "    embeddings[dir] = curr_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data Partitions\n",
    "To get a full glance at the embedding space's attributes we may want to look at the content of a review and how it relates to other reviews in the space so within this next code block we gather the nominal attributes of each review and load the three split parquets (train, test and validation or in our case unlabelled, labelled and validation) into the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTITIONS_DATA_DIR = os.path.abspath(os.path.join(parent_dir, DATA_DIR, 'partitions'))\n",
    "\n",
    "print(f\"Reading partitions data from {PARTITIONS_DATA_DIR}\")    \n",
    "\n",
    "partitions = {}\n",
    "\n",
    "for file in os.listdir(PARTITIONS_DATA_DIR):\n",
    "    if file.endswith('.parquet'):\n",
    "        filename = file.split('.')[0]\n",
    "        partitions[filename] = pd.read_parquet(os.path.join(PARTITIONS_DATA_DIR, file))\n",
    "    print(f'- Read {file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging content, title and label to embedding vectors\n",
    "The goal now is to merge the matrix representations with the dataframes. This will later on allow us to pass a dataset into Phoenix that contains the reviews content and its embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "merged_partitions = {}\n",
    "\n",
    "for embedding_model in embeddings:\n",
    "    print(f'Merging partitions for model {embedding_model}')\n",
    "    merged_list = []\n",
    "    \n",
    "    for partition_key, partition_df in partitions.items():\n",
    "        curr_partition_name = partition_key.split('_')[0]\n",
    "        matched = False\n",
    "        \n",
    "        for embedding_key, embedding_array in embeddings[embedding_model].items():\n",
    "            if curr_partition_name == embedding_key.split('_')[0]:\n",
    "                if isinstance(embedding_array, (list, pd.Series)):\n",
    "                    embedding_array = np.array(embedding_array)\n",
    "                \n",
    "                if len(partition_df) == embedding_array.shape[0]:\n",
    "                    partition_df = partition_df.copy()\n",
    "                    partition_df['embedding'] = embedding_array.tolist()\n",
    "                    merged_list.append(partition_df)\n",
    "                    matched = True\n",
    "                    print(f\"  - Merged {embedding_key} with {partition_key}\")\n",
    "                else:\n",
    "                    print(f\"  - Number of rows do not match for {embedding_key} and {partition_key}\")\n",
    "        \n",
    "        if not matched:\n",
    "            print(f\"  - No matching embedding found for {partition_key}\")\n",
    "    \n",
    "    if merged_list:\n",
    "        merged_partitions[embedding_model] = pd.concat(merged_list, ignore_index=True)\n",
    "    else:\n",
    "        print(f\"No partitions were merged for model {embedding_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Labeled data\n",
    "To project the high dimensional embeddings into a humanly readable format we implemented Arize's Phoenix app that allows us to interactively look at the embedding space projected down into 3 dimensions by UMAP.\n",
    "\n",
    "Additionally, it might be insightful to also look at a different dimension reduction approach - Therefore we made the `plot_pca` function which will project the embedding space into two dimensions using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def break_content(text, length=50):\n",
    "    lines = []\n",
    "    while len(text) > length:\n",
    "        space_index = text.rfind(' ', 0, length)\n",
    "        if space_index == -1:\n",
    "            space_index = length\n",
    "        lines.append(text[:space_index])\n",
    "        text = text[space_index:].lstrip()\n",
    "    lines.append(text)\n",
    "    return '<br>'.join(lines)\n",
    "\n",
    "\n",
    "def plot_pca(weak_labelled, key):\n",
    "    if key not in weak_labelled:\n",
    "        raise ValueError(f\"File {key} not found in the weak_labelled dictionary.\")\n",
    "\n",
    "    df = weak_labelled[key]\n",
    "\n",
    "    embeddings = np.vstack(df['embedding'].values)\n",
    "    content = df['content'].apply(lambda x: break_content(x)).values\n",
    "\n",
    "    pca = PCA(n_components=3)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "    pca_df = pd.DataFrame(reduced_embeddings, columns=['PCA1', 'PCA2', 'PCA3'])\n",
    "    pca_df['Content'] = content\n",
    "\n",
    "    fig = px.scatter_3d(pca_df, x='PCA1', y='PCA2', z='PCA3',\n",
    "                        title=f'PCA of Embedding Vectors for {key}',\n",
    "                        size_max=5, opacity=0.6, height=800,\n",
    "                        hover_data={'Content': True})\n",
    "    fig.update_traces(marker=dict(size=2))\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniLM Embedding Space\n",
    "First we will take a look at how the embedding space of the `mini-lm` embedding model looks.\n",
    "\n",
    "Note, to see the projected space in the Phoenix app, make sure to click the \"text_embedding\" link inside the app, this will load the 3-dimensional UMAP projection. Another thing to note is that UMAP in uses stochastic algorithms to speed up calculation so the representation you see may not look the same as we noted down so **this decomposition approach is non-deterministic**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.px_utils import create_dataset, launch_px\n",
    "\n",
    "mini_lm_ds = create_dataset('mini_lm', merged_partitions['mini_lm'], merged_partitions['mini_lm']['embedding'], content=merged_partitions['mini_lm']['content'])\n",
    "\n",
    "px_session = launch_px(mini_lm_ds, None)\n",
    "px_session.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding positions in the space are relatively clearly clustered. In this first example we are using the embedding vectors of huggingface's `all-MiniLM-L6-v2` BERT sentence transformer. This sentence transformer was trained on sentence pairs that appear as a Q&A. The resulting vector embedding therefore describes the semantic content of such a sentence - This is exactly what we can see in the embedding space; Reviews of the `amazon-polarity` dataset are clustered together according to their product niche, as for example already mentioned with the cluster containing music reviews.\n",
    "\n",
    "But we can also observe other semantic relationships:\n",
    "- Video game reviews lie between music and book reviews: This axis could perhaps describe interactivity; music can be enjoyed passively, games do have some interactions between cutscenes while books capture ones concentration and attention entirely.\n",
    "- Video game reviews lie opposite of tech gadgets and other devices: This axis might describe the abstraction of virtuality. Games are completely virtual tech while tech gadgets are physical devices.\n",
    "- Kid's toys are clustered between games and tech gadgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA of MiniLM\n",
    "Since Phoenix doesn't allow for a different dimension reduction technique we implement a PCA strategy ourselves. The UMAP technique differs vastly from PCA so looking at another technique could yield more interesting observations in the embedding space. PCA on the other hand is deterministic so the observations made may make more sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(merged_partitions, 'mini_lm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the UMAP representation the PCA reduction shows a triangular shaped embedding space, at each corner a cluster emerges. We can still roughly see the following four clusters:\n",
    "- Music albums\n",
    "- Books\n",
    "- Movies\n",
    "- Tech Gadgets\n",
    "\n",
    "So this visualization again support the claims made in the above analysis; The `all-MiniLM-L6-v2` clearly succeeds in embedding and clustering the reviews according to their semantic relatedness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mpnet-base Embedding Space\n",
    "Now we look at the embedding space of the `all-mpnet-base-v2` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpnet_base_ds = create_dataset('mpnet_base', merged_partitions['mpnet_base'], merged_partitions['mpnet_base']['embedding'], content=merged_partitions['mpnet_base']['content'])\n",
    "\n",
    "px_session = launch_px(mpnet_base_ds, None)\n",
    "px_session.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UMAP projection of `mpnet_base` as seen in Phoenix also shows roughly the same clusters as the UMAP projection of the `mini_lm` embedding space.\n",
    "The main and most obvious sights stay the same as already noted in the previous exploration on the `mini_lm`'s decomposition:\n",
    "- One cluster that separates itself from the other points in the space is the music-related cluster\n",
    "- On the other  side of the space much data points seem to be about books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA of mpnet-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(merged_partitions, 'mpnet_base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `mpnet_base` PCA projection shows a decomposed space similar to the PCA of the `mini_lm` embedding model. This observation makes sense because both embedding models were trained with similar BERT-style objectives focused on mapping and clustering the semantic meanings of sentences. Consequently, the decomposed spaces map similar variances onto the principal components.\n",
    "\n",
    "A confusion that might arise is the fact that when comparing both 3D-PCA plots the principal component #2 seems to be flipped. This does not change the components meaning since the principal components derived from PCA are unique up to a sign flip. This is because the eigenvectors of a covariance matrix (which define the principal components) can point in either direction along the axis they define. Both directions represent the same principal component, just with inverted signs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
