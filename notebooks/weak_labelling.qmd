---
title: Weak Labeling for Semi-Supervised Learning
jupyter: python3
---

This notebook demonstrates how to use weak labeling for semi-supervised learning. We will explore different weak labeling techniques and evaluate their performance.

The weak labeling techniques we will explore are:

1. KNeighbors
2. Logistic Regression
3. Random Forest
4. Neural Network
5. Support Vector Machine

We will use the `sentence-transformers` library to generate sentence embeddings and the `scikit-learn` library to train the weak labeling models.

The weak labeling models will be trained on the labeled development set and will then be used to predict the labels of the unlabeled development set. We will then evaluate the performance of the weak labeling models on the validation set, which is also labeled.

Finally, we will save the best weak labeling model so that it can be used for the semi-supervised learning phase in the weak labeling pipeline.

## Setup

```{python}
#| ExecuteTime: {end_time: '2024-06-04T11:10:34.560421Z', start_time: '2024-06-04T11:10:33.331443Z'}
import os
import pickle
import random
import sys

import numpy as np
import seaborn as sns
import torch
from dotenv import load_dotenv
from matplotlib import pyplot as plt
from sentence_transformers import SentenceTransformer
from sklearn.metrics import classification_report, confusion_matrix

current_dir = os.getcwd()
parent_dir = os.path.dirname(current_dir)

sys.path.append(parent_dir)

load_dotenv()

DATA_DIR = os.getenv('DATA_DIR', 'data')
MODELS_DIR = os.getenv('MODELS_DIR', 'models')

DATA_DIR = os.path.abspath(os.path.join(parent_dir, DATA_DIR))
MODELS_DIR = os.path.abspath(os.path.join(parent_dir, MODELS_DIR))

assert DATA_DIR is not None
assert MODELS_DIR is not None

SEED = 1337


def set_seed():
    torch.use_deterministic_algorithms(True)

    random.seed(SEED)
    np.random.seed(SEED)

    torch.manual_seed(SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(SEED)
        torch.cuda.manual_seed_all(SEED)

    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


set_seed()
```

As the `sentence-transformers` gives us a `torch` model, we need to check the environment to see if we can use a suitable accelerator.

```{python}
#| ExecuteTime: {end_time: '2024-06-04T11:10:34.600541Z', start_time: '2024-06-04T11:10:34.561403Z'}
if torch.backends.mps.is_available():
    device = torch.device('mps')
elif torch.cuda.is_available():
    device = torch.device('cuda')
else:
    device = torch.device('cpu')

print('Device:', device)
```

## Load Datasets

We will load the labeled development set, the unlabeled development set, and the validation set.

Here a short overview of the datasets:
- Train Dataset: Labeled development set
- Validation Dataset: Validation set
- Test Dataset: Unlabeled development set

The test dataset would not be available in a real-world scenario, but we will use it to get an idea of how well the weak labeling models perform on unseen data and thus make assumptions about the performance later on the downstream task.

```{python}
#| ExecuteTime: {end_time: '2024-06-04T11:10:34.695770Z', start_time: '2024-06-04T11:10:34.601495Z'}
from src.data_loader import load_datasets

# print current path of data dir
print(DATA_DIR)

partitions_dir = os.path.join(DATA_DIR, 'partitions')
labelled_dev, unlabelled_dev, val_set = load_datasets(partitions_dir)
train_df = labelled_dev
y_train = train_df['label']

val_df = val_set
y_val = val_df['label']

test_df = unlabelled_dev
y_test = test_df['ground_truth']
```

## Sentence Embeddings

We will use the `sentence-transformers` library to generate sentence embeddings for the text data. The embedding models we will discuss are:
1. `all-MiniLM-L6-v2`: Fast and efficient model
2. `all-mpnet-base-v2`: High-quality model

All the models are pre-trained on a large corpus of text data and can generate high-quality sentence embeddings. The choice of the model depends on the trade-off between quality and efficiency. From the documentation:

> The all-mpnet-base-v2 model provides the best quality, while all-MiniLM-L6-v2 is 5 times faster and still offers good quality.

In the next section, we will compare the impact on performance of the weak labeling models when using the two different embedding models and ultimately make a decision on which model to use.

```{python}
#| ExecuteTime: {end_time: '2024-06-04T11:10:39.128944Z', start_time: '2024-06-04T11:10:34.697344Z'}


def get_sentence_transformer_model(model_name, device=device):
    return SentenceTransformer(model_name).to(device)


EMBEDDING_MODELS = {
    'mini_lm': get_sentence_transformer_model("all-MiniLM-L6-v2"),
    'mpnet_base': get_sentence_transformer_model("all-mpnet-base-v2")
}
```

The functions below will be used to generate and save the sentence embeddings for the text data using the sentence-transformers model. The embeddings will be generated for the labeled development set, the unlabeled development set, and the validation set. The embeddings will be saved to disk so that they can be loaded later without having to regenerate them. 

If the embeddings are already generated and saved to disk, the functions will load the embeddings from disk instead of regenerating them.

```{python}
#| ExecuteTime: {end_time: '2024-06-04T11:10:39.147032Z', start_time: '2024-06-04T11:10:39.129974Z'}
import time

EMBEDDINGS_FOLDER = os.path.join(DATA_DIR, 'embeddings')
os.makedirs(EMBEDDINGS_FOLDER, exist_ok=True)


def load_embeddings(filename):
    """Load the embeddings from disk."""
    with open(f'{EMBEDDINGS_FOLDER}/{filename}.pkl', 'rb') as f:
        embeddings = pickle.load(f)
    return embeddings


def generate_embeddings(model, texts, verbose=True):
    """Generate sentence embeddings using the sentence-transformers model."""
    embeddings = model.encode(texts, show_progress_bar=verbose)
    return embeddings


def save_embeddings(embeddings, filename):
    """Save the embeddings to disk."""
    with open(f'{EMBEDDINGS_FOLDER}/{filename}.pkl', 'wb') as f:
        pickle.dump(embeddings, f)


def generate_and_save_embeddings(model, texts, filename):
    """Generate and save the embeddings."""
    if os.path.exists(f'{EMBEDDINGS_FOLDER}/{filename}.pkl'):
        embeddings = load_embeddings(filename)
        return embeddings

    embeddings = generate_embeddings(model, texts)
    save_embeddings(embeddings, filename)
    return embeddings


EMBEDDING_MODEL_DATA = {}
for model_name, model in EMBEDDING_MODELS.items():
    print(f'Generating embeddings for {model_name}')

    start = time.time()

    X_train = generate_and_save_embeddings(model, train_df['content'].values,
                                           f'{model_name}/train_embeddings')
    X_val = generate_and_save_embeddings(model, val_df['content'].values,
                                         f'{model_name}/val_embeddings')
    X_test = generate_and_save_embeddings(model, test_df['content'].values,
                                          f'{model_name}/test_embeddings')

    end = time.time()

    print(f'Generated embeddings for {model_name} in {end - start:.2f} seconds')
    print(
        f'Train Embeddings Shape: {X_train.shape}, Validation Embeddings Shape: {X_val.shape}, Test Embeddings Shape: {X_test.shape}')

    EMBEDDING_MODEL_DATA[model_name] = {
        'X_train': X_train,
        'X_val': X_val,
        'X_test': X_test
    }
```

As expected the larger model `all-mpnet-base-v2` takes more time to generate the embeddings compared to the smaller model `all-MiniLM-L6-v2`. The dimensions of the embeddings also differ, with the larger model generating embeddings of size 768 and the smaller model generating embeddings of size 384.

## Evaluation Function

We will use the following function to evaluate the performance of the weak labeling models on the validation and test sets. It will print the classification report and confusion matrix for the predictions made by the weak labeling models.

The test set would not be available in a real-world scenario, but we will use it to get an idea of how well the weak labeling models perform on unseen data and thus make assumptions about the performance later on the downstream task. This obviously introduces some bias for the decision-making process, but we will use it for the sake of the demonstration.

```{python}
#| ExecuteTime: {end_time: '2024-06-04T11:10:39.149989Z', start_time: '2024-06-04T11:10:39.147809Z'}
def print_evaluation(y_true, y_pred, title):
    """
    Prints the evaluation metrics for a classification model.
    """
    print(f'\nClassification Report: {title}')
    print(classification_report(y_true, y_pred))

    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix: {title}')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()
```

## Comparison between Embedding Models

We discovered that the difference in the embeddings generated by the two models significantly affects the performance of the weak labeling models. For this reason, we will compare the performance of the weak labeling models trained on the embeddings generated by the two models to get an idea of which model performs better. We will use a simple Logistic Regression model for this comparison.

As this is a baseline comparison, we will not perform hyperparameter tuning for the Logistic Regression model.

```{python}
#| ExecuteTime: {end_time: '2024-06-04T11:10:39.773102Z', start_time: '2024-06-04T11:10:39.150698Z'}
from sklearn.linear_model import LogisticRegression

for model_name, model_data in EMBEDDING_MODEL_DATA.items():
    X_train = model_data['X_train']
    X_val = model_data['X_val']
    X_test = model_data['X_test']

    log_reg = LogisticRegression(max_iter=100_000)
    log_reg.fit(X_train, y_train)

    print_evaluation(y_val, log_reg.predict(X_val),
                     f'Logistic Regression ({model_name}) (Validation Set)')

    print_evaluation(y_test, log_reg.predict(X_test),
                     f'Logistic Regression ({model_name}) (Test Set)')
```

We can see that for this baseline comparison the `all-mpnet-base-v2` model performs better than the `all-MiniLM-L6-v2` model with a considerable margin of roughly $6\%$ in the F1-score on both the validation and test sets.

As the `all-mpnet-base-v2` model performs better than the `all-MiniLM-L6-v2` model, we will use the embeddings generated by the `all-mpnet-base-v2` model for the subsequent steps.

```{python}
#| ExecuteTime: {end_time: '2024-06-04T11:10:39.822435Z', start_time: '2024-06-04T11:10:39.801744Z'}
# set X_train, X_val, X_test to the embeddings of the best model for subsequent steps
X_train = EMBEDDING_MODEL_DATA['mpnet_base']['X_train']
y_train = train_df['label']

X_val = EMBEDDING_MODEL_DATA['mpnet_base']['X_val']
y_val = val_df['label']

X_test = EMBEDDING_MODEL_DATA['mpnet_base']['X_test']
y_test = test_df['ground_truth']
```

## Approach 1: Logistic Regression

As the tasks is a binary classification task, it naturally lends itself to logistic regression. The logistic regression model is simple and interpretable, and it can serve as a good baseline for the weak labeling task. 

We will use grid search to find the best hyperparameters for the logistic regression model as we will do for all the other subsequent approaches discussed in this notebook.

```{python}
#| ExecuteTime: {end_time: '2024-06-04T11:10:41.223932Z', start_time: '2024-06-04T11:10:39.826867Z'}
from sklearn.model_selection import GridSearchCV

log_reg_param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 3, 5, 10, 100],
    'max_iter': [100_000]
}

log_reg_grid_search = GridSearchCV(LogisticRegression(),
                                   log_reg_param_grid,
                                   n_jobs=-1,
                                   cv=5,
                                   verbose=3)
log_reg_grid_search.fit(X_train, y_train)

log_reg_model = log_reg_grid_search.best_estimator_
print(f'Best Parameters: {log_reg_grid_search.best_params_}')

print_evaluation(y_val, log_reg_model.predict(X_val),
                 'Logistic Regression (Validation Set)')

print_evaluation(y_test, log_reg_model.predict(X_test),
                 'Logistic Regression (Test Set)')
```

We see no real improvement in the performance of the logistic regression model compared to the baseline comparison. This is expected as the logistic regression model is simple and linear, and it might not be able to capture the complex relationships in the data. This does also not change with hyperparameter tuning. However, the F1-score on the validation set is still around $0.86$ which is quite good for a simple model like logistic regression.

This performance is further confirmed on the test set where the F1-score is also around $0.86$ which is a good sign that the model is generalizing well to unseen data.

## Approach 2: KNeighbors Weak Labeling

We will use the KNeighbors classifier from the `scikit-learn` library to train a weak labeling model on the labeled development set. KNneighbors is a simple and effective algorithm for classification tasks, and it can be used for weak labeling as well as the notion of clustering is similar to weak labeling in the sense that we are trying to find the nearest neighbors for a given sentence embedding in the feature space.

As there are many hyperparameters to tune, we will use grid search to find the best hyperparameters for the KNeighbors classifier.

```{python}
#| ExecuteTime: {end_time: '2024-06-04T11:13:48.888192Z', start_time: '2024-06-04T11:10:41.231973Z'}
from sklearn.neighbors import KNeighborsClassifier

param_grid = {
    'n_neighbors': [3, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski'],
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
    'leaf_size': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],
}

grid_search = GridSearchCV(KNeighborsClassifier(),
                           param_grid,
                           n_jobs=-1,
                           cv=5,
                           verbose=3)

grid_search.fit(X_train, y_train)
best_knn = grid_search.best_estimator_

print_evaluation(y_val, best_knn.predict(X_val),
                 'KNeighbors Weak Labeling (Validation Set)')

print_evaluation(y_test, best_knn.predict(X_test),
                 'KNeighbors Weak Labeling (Test Set)')
```

## Approach 2.1 KNeighbors Weak Labeling on PCA reduced embeddings
It is known that K-Nearest Neighbor models struggle with high-dimensional data. We will use PCA to reduce the dimensionality of the sentence embeddings and then train a KNeighbors classifier on the reduced embeddings.

```{python}
#| ExecuteTime: {end_time: '2024-06-04T11:13:58.731093Z', start_time: '2024-06-04T11:13:48.889480Z'}
from sklearn.decomposition import PCA

pca = PCA(n_components=8)
X_train_pca = pca.fit_transform(X_train)

grid_search.fit(X_train_pca, y_train)
best_knn_pca = grid_search.best_estimator_

print_evaluation(y_val, best_knn_pca.predict(pca.transform(X_val)),
                 'KNeighbors Weak Labeling (PCA Reduced) (Validation Set)')

print_evaluation(y_test, best_knn_pca.predict(pca.transform(X_test)),
                 'KNeighbors Weak Labeling (PCA Reduced) (Test Set)')
```

## Approach 3: Random Forest

```{python}
#| jupyter: {is_executing: true}
#| ExecuteTime: {start_time: '2024-06-04T11:14:10.833420Z'}
from sklearn.ensemble import RandomForestClassifier

gradient_boosting_param_grid = {
    'n_estimators': [100, 500, 1000],
    'max_depth': [3, 5, 7, 9, 11, 13],
    'max_features': [None, 'sqrt', 'log2']
}

rf_grid_search = GridSearchCV(RandomForestClassifier(),
                              gradient_boosting_param_grid,
                              n_jobs=-1,
                              cv=5,
                              verbose=3)

rf_grid_search.fit(X_train, y_train)

rf_model = rf_grid_search.best_estimator_
print(f'Best Parameters: {rf_grid_search.best_params_}')

print_evaluation(y_val, rf_model.predict(X_val),
                 'Random Forest (Validation Set)')

print_evaluation(y_test, rf_model.predict(X_test),
                 'Random Forest (Test Set)')
```

## Approach 4: Neural Network

```{python}
#| jupyter: {is_executing: true}
from sklearn.neural_network import MLPClassifier

mlp_param_grid = {
    'hidden_layer_sizes': [(512, 128), (256, 64), (128, 32)],
    'max_iter': [1000],
}

mlp_grid_search = GridSearchCV(MLPClassifier(),
                               mlp_param_grid,
                               n_jobs=-1,
                               cv=5,
                               verbose=3)
mlp_grid_search.fit(X_train, y_train)
mlp_model = mlp_grid_search.best_estimator_
print(f'Best Parameters: {mlp_grid_search.best_params_}')

print_evaluation(y_val, mlp_model.predict(X_val),
                 'Neural Network (Validation Set)')

print_evaluation(y_test, mlp_model.predict(X_test),
                 'Neural Network (Test Set)')
```

## Approach 5: Support Vector Machine

```{python}
#| jupyter: {is_executing: true}
from sklearn.svm import SVC

svm_param_grid = {
    'C': [0.0001, 0.001, 0.01, 0.1, 1, 3, 5, 10],
    'gamma': [1, 0.1, 0.01, 0.001],
    'kernel': ['rbf', 'linear', 'poly', 'sigmoid']
}

svm_grid_search = GridSearchCV(SVC(),
                               svm_param_grid,
                               n_jobs=-1,
                               cv=5,
                               verbose=3)

svm_grid_search.fit(X_train, y_train)
svm_model = svm_grid_search.best_estimator_
print(f'Best Parameters: {svm_grid_search.best_params_}')

print_evaluation(y_val, svm_model.predict(X_val),
                 'Support Vector Machine (Validation Set)')

print_evaluation(y_test, svm_model.predict(X_test),
                 'Support Vector Machine (Test Set)')
```

## Comparison and Conclusion of Weak Labeling Approaches

We have trained and evaluated five different weak labeling models on the validation and test sets. The performance of the weak labeling models on the validation set is as follows:

1. Logistic Regression: F1-score of around $0.86$
2. KNeighbors: F1-score of around $0.86$
3. KNeighbors (PCA Reduced): F1-score of around $0.86$
4. Random Forest: F1-score of around $0.86$
5  Neural Network: F1-score of around $0.86$
6. Support Vector Machine: F1-score of around $0.86$



## Save Best Models

We will save the best weak labeling models so that they can be used for the semi-supervised learning phase in the weak labeling pipeline.

```{python}
#| jupyter: {is_executing: true}
MODELS_FOLDER = os.getenv('MODELS_DIR', 'models')
os.makedirs(MODELS_FOLDER, exist_ok=True)


def save_model(model, filename):
    with open(f'{MODELS_FOLDER}/weak_labelling/{filename}.pkl', 'wb') as f:
        pickle.dump(model, f)
```

```{python}
#| jupyter: {is_executing: true}
save_model(best_knn, 'knn')
save_model(log_reg_model, 'log_reg')
save_model(rf_model, 'rf')
save_model(mlp_model, 'mlp')
save_model(svm_model, 'svm')
```

